{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "\n",
    "1. search for 'verilog' repos;\n",
    "2. keep repo with permissive license;\n",
    "3. download repo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from github import Github\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import  os\n",
    "ACCESS_TOKEN = os.getenv('git_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'limit': 5000, 'used': 14, 'remaining': 4986, 'reset': 1712741224}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_rate_limit().core.raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stall requesting when reaching the limit (5000 requests per hour)\n",
    "def pre_github_request_checker():\n",
    "    rate_data = g.get_rate_limit().core.raw_data\n",
    "    if rate_data['remaining'] < 50:\n",
    "        time_to_reset = rate_data['reset'] - int(time.time()) + 1\n",
    "        print(f\"Sleeping for {time_to_reset} seconds\")\n",
    "        time.sleep(time_to_reset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 repos for: Verilog, 1980-01-01..2010-01-01\n",
      "Found 1000 repos for: Verilog, 1980-01-01..2015-01-01\n"
     ]
    }
   ],
   "source": [
    "def search_github(language, start_date, end_date, permissible_lang_list=['Verilog', 'SystemVerilog']):\n",
    "    \"\"\"\n",
    "    More info:\n",
    "    https://docs.github.com/en/search-github/getting-started-with-searching-on-github/understanding-the-search-syntax#query-for-dates\n",
    "    \"\"\"\n",
    "    assert language in permissible_lang_list\n",
    "    date_q = f\"{start_date.strftime('%Y-%m-%d')}..{end_date.strftime('%Y-%m-%d')}\" \n",
    "    result = g.search_repositories(\"\",language=language, created=date_q) \n",
    "    print(f\"Found {result.totalCount} repos for: {language}, {date_q}\")\n",
    "    return result\n",
    "\n",
    "# results = search_github('Verilog', datetime(1980,1,1), datetime(2010,1,1))\n",
    "# results = search_github('Verilog', datetime(1980,1,1), datetime(2015,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repo_to_df(df, repo, data_list):\n",
    "    data = [getattr(repo, attr) for attr in data_list]\n",
    "    data.append(repo.get_topics())\n",
    "    try:\n",
    "        license_url = repo.get_license().license.url\n",
    "    except:\n",
    "        license_url = \"None\"\n",
    "    data.append(license_url)\n",
    "    df.loc[len(df)] = data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get repo info, add to df\n",
    "def process_repo_search_results(df, results, data_list):\n",
    "    for i in range(1000):\n",
    "        rate_data = g.get_rate_limit().core.raw_data\n",
    "        now_seconds = int(time.time())\n",
    "        if rate_data['remaining'] < 100:\n",
    "            time_to_reset = rate_data['reset'] - int(time.time()) + 1\n",
    "            print(f\"Sleeping for {time_to_reset} seconds\")\n",
    "            time.sleep(time_to_reset)\n",
    "        page = results.get_page(i)\n",
    "        page_size = len(page)\n",
    "        for j in range(page_size):\n",
    "            df = add_repo_to_df(df, page[j], data_list)\n",
    "        if page_size < 30:\n",
    "            break   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repos(df, language, start_date, end_date, data_list):\n",
    "    # search by lang, start & end dates\n",
    "    repo_search_results = search_github(language, start_date, end_date)\n",
    "    if repo_search_results.totalCount > 0:\n",
    "        if repo_search_results.totalCount >= 1000:\n",
    "            # Reduce date range recursively\n",
    "            delta = (end_date - start_date) / 2\n",
    "            find_repos(df, language,start_date, end_date - delta, data_list)\n",
    "            find_repos(df, language, start_date + delta, end_date, data_list)\n",
    "        else:\n",
    "            process_repo_search_results(df, repo_search_results, data_list)\n",
    "    print(f\"Done: {start_date.strftime('%Y-%m-%d')}..{end_date.strftime('%Y-%m-%d')}, df length: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [clone_url, created_at, description, forks_count, full_name, id, language, name, size, stargazers_count, updated_at, topics, license_url]\n",
      "Index: []\n",
      "Found 15 repos for: Verilog, 1980-01-01..2010-01-01\n",
      "Done: 1980-01-01..2010-01-01, df length: 15\n"
     ]
    }
   ],
   "source": [
    "data_list = [\n",
    "    \"id\",\n",
    "    \"clone_url\",\n",
    "    \"created_at\",\n",
    "    \"description\",\n",
    "    \"full_name\",\n",
    "    \"language\",\n",
    "    \"name\",\n",
    "    \"size\",\n",
    "    \"stargazers_count\",\n",
    "    \"updated_at\",\n",
    "    \"forks_count\"\n",
    "]\n",
    "data_list.sort()\n",
    "\n",
    "df = pd.DataFrame(columns=data_list + [\"topics\",\"license_url\"])\n",
    "\n",
    "language = \"Verilog\"\n",
    "# You can split up the search to make it more manageable by splitting your search over certain years\n",
    "start_date = datetime(1980,1,1)\n",
    "end_date = datetime.now()\n",
    "# end_date = datetime(2010,1,1)\n",
    "print(df)\n",
    "find_repos(df, language, start_date, end_date, data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the rate limit is 5000 request per hour, so for 51321 it needs 10 hours + some overhead on search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clone_url</th>\n",
       "      <th>created_at</th>\n",
       "      <th>description</th>\n",
       "      <th>forks_count</th>\n",
       "      <th>full_name</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>stargazers_count</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>topics</th>\n",
       "      <th>license_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/m-labs/milkymist.git</td>\n",
       "      <td>2009-08-17 14:53:55+00:00</td>\n",
       "      <td>SoC design for Milkymist One - LM32, DDR SDRAM...</td>\n",
       "      <td>41</td>\n",
       "      <td>m-labs/milkymist</td>\n",
       "      <td>279998</td>\n",
       "      <td>Verilog</td>\n",
       "      <td>milkymist</td>\n",
       "      <td>34543</td>\n",
       "      <td>147</td>\n",
       "      <td>2024-02-12 07:02:03+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/steveicarus/ivtest.git</td>\n",
       "      <td>2009-03-04 23:55:23+00:00</td>\n",
       "      <td>Regression test suite for Icarus Verilog. (OBS...</td>\n",
       "      <td>51</td>\n",
       "      <td>steveicarus/ivtest</td>\n",
       "      <td>143278</td>\n",
       "      <td>Verilog</td>\n",
       "      <td>ivtest</td>\n",
       "      <td>13163</td>\n",
       "      <td>116</td>\n",
       "      <td>2023-12-29 18:34:55+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://api.github.com/licenses/gpl-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   clone_url                created_at  \\\n",
       "0    https://github.com/m-labs/milkymist.git 2009-08-17 14:53:55+00:00   \n",
       "1  https://github.com/steveicarus/ivtest.git 2009-03-04 23:55:23+00:00   \n",
       "\n",
       "                                         description  forks_count  \\\n",
       "0  SoC design for Milkymist One - LM32, DDR SDRAM...           41   \n",
       "1  Regression test suite for Icarus Verilog. (OBS...           51   \n",
       "\n",
       "            full_name      id language       name   size  stargazers_count  \\\n",
       "0    m-labs/milkymist  279998  Verilog  milkymist  34543               147   \n",
       "1  steveicarus/ivtest  143278  Verilog     ivtest  13163               116   \n",
       "\n",
       "                 updated_at topics                              license_url  \n",
       "0 2024-02-12 07:02:03+00:00     []                                     None  \n",
       "1 2023-12-29 18:34:55+00:00     []  https://api.github.com/licenses/gpl-2.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data', exist_ok=True)\n",
    "df.to_csv(f\"data/{language}_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [clone_url, created_at, description, forks_count, full_name, id, language, name, size, stargazers_count, updated_at, topics, license_url]\n",
      "Index: []\n",
      "Found 4 repos for: SystemVerilog, 1980-01-01..2012-01-01\n",
      "Done: 1980-01-01..2012-01-01, df length: 4\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.DataFrame(columns=data_list + [\"topics\",\"license_url\"])\n",
    "language = 'SystemVerilog'\n",
    "print(df2)\n",
    "df2 = find_repos(df2, language, start_date, end_date, data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(f\"data/{language}_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concate repo dfs and deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_deduplicate_gh_search_results(csvs):\n",
    "    df = pd.concat(map(lambda x: pd.read_csv(x, na_values=['None']), csvs), ignore_index=True)\n",
    "    df = df.drop(['Unnamed: 0'],axis=1)\n",
    "    df = df.drop_duplicates([c for c in df.columns if c != 'updated_at'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from os import path as osp\n",
    "import os\n",
    "\n",
    "v_filepaths = ['./Verilog_1980-01-01_2012-01-01.csv']\n",
    "sv_filepaths = ['./SystemVerilog_1980-01-01_2012-01-01.csv']\n",
    "\n",
    "verilog_df = combine_and_deduplicate_gh_search_results(v_filepaths)\n",
    "systemverilog_df = combine_and_deduplicate_gh_search_results(sv_filepaths)\n",
    "\n",
    "print(len(verilog_df))\n",
    "print(len(systemverilog_df))\n",
    "\n",
    "repo_indices_dir = 'data/search_repo_indices'\n",
    "os.makedirs(repo_indices_dir)\n",
    "verilog_df.to_csv(osp.join(repo_indices_dir,\"full_verilog_repos.csv\"))\n",
    "systemverilog_df.to_csv(osp.join(repo_indices_dir,\"full_systemverilog_repos.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([verilog_df,systemverilog_df]).drop_duplicates(subset=[c for c in verilog_df.columns if not c in ['language']])\n",
    "all_df.to_csv(\"data/all_deduplicated_repos.csv\")\n",
    "print(len(all_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define extensions to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.intel.com/content/www/us/en/programmable/quartushelp/17.0/reference/glossary/glosslist.htm\n",
    "# https://marketplace.visualstudio.com/items?itemName=eirikpre.systemverilog\n",
    "verilog_extension_files = ['v','verilog','vlg','vh']\n",
    "system_verilog_extension_files = ['sv','svh','svp', 'sva']\n",
    "extra_file_types = ['vo','vt'] # verilog output, verilog test bench\n",
    "\n",
    "extensions_to_keep = verilog_extension_files + system_verilog_extension_files + extra_file_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding licenses and remove non permissive repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_licenses_from_repo_df_to_dict(repo_df, licenses_dict):\n",
    "    df_with_unique_licenses = repo_df.loc[repo_df['license_url'].dropna().drop_duplicates().index]\n",
    "    repo_ids = list(df_with_unique_licenses['id'])\n",
    "    for rid in repo_ids:\n",
    "        pre_github_request_checker()\n",
    "        license_data = g.get_repo(rid).get_license().license.raw_data\n",
    "        licenses_dict[license_data['url']] = license_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "licenses_dict = {}\n",
    "add_licenses_from_repo_df_to_dict(verilog_df,licenses_dict)\n",
    "add_licenses_from_repo_df_to_dict(systemverilog_df,licenses_dict)\n",
    "df = pd.DataFrame.from_dict(licenses_dict, orient='index')\n",
    "df.to_csv(os.path.join('data/search_repo_indices','licenses.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_licenses_with_permissions_conditions(license_df,permissions=[],conditions=[]):\n",
    "    indices = []\n",
    "    for i, row in license_df.iterrows():\n",
    "        if len(permissions) == 0 or set(permissions).issubset(set(row['permissions'])):\n",
    "            if len(conditions) == 0 or len(set(conditions).intersection(set(row['conditions']))) > 0 :\n",
    "                indices.append(i)\n",
    "    return license_df.loc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "permissions = ['modifications','distribution']\n",
    "special_conditions = ['same-license--file','same-license--library','same-license']\n",
    "permissive_licenses_df = get_licenses_with_permissions_conditions(df,permissions=permissions,conditions=[])\n",
    "distributive_licenses_df = get_licenses_with_permissions_conditions(df,permissions=['distribution'],conditions=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(permissive_licenses_df))\n",
    "print(len(distributive_licenses_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion, all licenses found are permissive in that they allow modifications and distribution! Repos without licenses are not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sv_df = systemverilog_df.dropna(subset=['license_url'])\n",
    "p_ve_df = verilog_df.dropna(subset=['license_url'])\n",
    "\n",
    "p_sv_df.to_csv(os.path.join(repo_indices_dir,\"permissive_systemverilog_repos.csv\"))\n",
    "p_ve_df.to_csv(os.path.join(repo_indices_dir,\"permissive_verilog_repos.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_all_df = pd.concat([p_ve_df,p_sv_df]).drop_duplicates(subset=[c for c in verilog_df.columns if not c in ['language']])\n",
    "p_all_df.to_csv(\"data/permissive_all_deduplicated_repos.csv\")\n",
    "print(len(p_all_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)\n",
    "all_permissions = []\n",
    "all_conditions = []\n",
    "all_limitations = []\n",
    "for i,row in df.iterrows():\n",
    "    all_permissions.append(row['permissions'])\n",
    "    all_conditions.append(row['conditions'])\n",
    "    all_limitations.append(row['limitations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['include-copyright', 'document-changes', 'disclose-source', 'same-license'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['include-copyright', 'document-changes', 'disclose-source', 'same-license'],\n",
       " ['include-copyright'],\n",
       " ['include-copyright'],\n",
       " ['include-copyright',\n",
       "  'disclose-source',\n",
       "  'document-changes',\n",
       "  'same-license--library'],\n",
       " ['include-copyright',\n",
       "  'disclose-source',\n",
       "  'document-changes',\n",
       "  'same-license--library'],\n",
       " [],\n",
       " ['include-copyright'],\n",
       " ['include-copyright',\n",
       "  'document-changes',\n",
       "  'disclose-source',\n",
       "  'network-use-disclose',\n",
       "  'same-license'],\n",
       " [],\n",
       " ['include-copyright'],\n",
       " [],\n",
       " ['disclose-source', 'include-copyright', 'same-license--file'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['include-copyright', 'document-changes', 'same-license'],\n",
       " ['disclose-source', 'include-copyright', 'same-license'],\n",
       " ['include-copyright--source'],\n",
       " ['include-copyright--source', 'document-changes'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['disclose-source', 'include-copyright', 'same-license'],\n",
       " ['include-copyright']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conditions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download repos:\n",
    "\n",
    "There are two ways:\n",
    "1. Clone or download full repo, then filter out files;\n",
    "2. get into repos and download single files if its extension is the target ones.\n",
    "\n",
    "The first method requires more storage space and network usage, but the second method requires more github api requests (for each file, it requires one get_repo search + n get_contents search).\n",
    "\n",
    "therefore, the first method is prefered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. raw git clone\n",
    "\n",
    "1.1 create downloading bash script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'git clone --depth 1 --no-tags https://github.com/mrehkopf/sd2snes.git ./data/full_repos/exp'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_clone_command(clone_url, directory, depth=1, branch='master'):\n",
    "    command_parts = [f'git clone']\n",
    "    # TODO: rerun search without master branch (causes error for some searches!)\n",
    "    # if branch is not set, then the clone method will download default branch\n",
    "    # command_parts.append(f\"-b {branch}\") \n",
    "    command_parts.append(f\"--depth {depth}\")\n",
    "    command_parts.append(f\"--no-tags\")\n",
    "    # command_parts.append(f\"--no-checkout\")\n",
    "    command_parts.append(clone_url)\n",
    "    command_parts.append(directory)\n",
    "    return \" \".join(command_parts)\n",
    "\n",
    "def create_clone_script_for_df(repo_df,script_out_path,clone_out_dir):\n",
    "    with open(script_out_path,'w+') as f:\n",
    "        for i,row in repo_df.iterrows():\n",
    "            clone_url = row['clone_url']\n",
    "            out_dir = os.path.join(clone_out_dir,str(row['id']))\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.mkdir(out_dir)\n",
    "            f.write(create_clone_command(clone_url,str(os.path.abspath(out_dir))).replace(\"\\\\\",\"/\") + \"\\n\")\n",
    "\n",
    "create_clone_command(\"https://github.com/mrehkopf/sd2snes.git\",\"./data/full_repos/exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/full_repos/permissive\", exist_ok=True)\n",
    "create_clone_script_for_df(p_all_df, \"./data/clone_all_p.sh\",\"data/full_repos/permissive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash data/clone_all_p.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Filter repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument '1' in rsplit can limit the split to max 2 segments\n",
    "def get_file_extension(path):\n",
    "    return path.rsplit(\".\",1)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_files_without_right_extension(start_dir, extensions_to_keep):\n",
    "    errors = []\n",
    "    for root, dirs, files in os.walk(start_dir):\n",
    "        for file in [sf for sf in files if not get_file_extension(sf) in extensions_to_keep]:\n",
    "            file_path = os.path.join(root,file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "            except Exception as e:\n",
    "                errors.append(e)\n",
    "    return errors\n",
    "\n",
    "def delete_empty_dirs(start_dir):\n",
    "    errors = []\n",
    "    for root, dirs, files in os.walk(start_dir,topdown=False):\n",
    "        for d in dirs:\n",
    "            dir_path = os.path.join(root,d)\n",
    "            if len(os.listdir(dir_path)) == 0:\n",
    "                try:\n",
    "                    os.rmdir(dir_path)\n",
    "                except Exception as e:\n",
    "                    errors.append(e)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dir = \"data/full_repos/permissive\"\n",
    "\n",
    "files_errors = delete_all_files_without_right_extension(start_dir,extensions_to_keep)\n",
    "dirs_errors = delete_empty_dirs(start_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. fine-grained repo download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_files_dict(count, content):\n",
    "    global files_dict\n",
    "    files_dict[count] = {\n",
    "        \"path\": content.raw_data['path'],\n",
    "        \"size\": content.raw_data['size'],\n",
    "        \"count_id\": count\n",
    "    }\n",
    "\n",
    "def download_content(repo,content,extensions,out_dir):\n",
    "    content_raw_data = content.raw_data\n",
    "    content_type = content_raw_data['type']\n",
    "    if content_type == 'dir':\n",
    "        pre_github_request_checker()\n",
    "        new_contents = repo.get_contents(content_raw_data['path'])\n",
    "        for new_content in new_contents:\n",
    "            download_content(repo,new_content,extensions,out_dir)\n",
    "    elif content_type == 'file':\n",
    "        extension = get_file_extension(content_raw_data['name'])\n",
    "        if extension in extensions:\n",
    "            global file_count\n",
    "            update_files_dict(file_count,content)\n",
    "            pre_github_request_checker()\n",
    "            try:\n",
    "                with open(os.path.join(out_dir,str(file_count) + \".\" + extension),'wb') as f:\n",
    "                    f.write(content.decoded_content)\n",
    "                file_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Caught exception while trying to write content:\\n{e}\")\n",
    "    # raise Exception(f\"Content type not recognized: {content_type}\")\n",
    "\n",
    "def download_files_from_repo(repo,extensions,out_dir):\n",
    "    global files_dict\n",
    "    global file_count\n",
    "    file_count = 0\n",
    "    files_dict = {}\n",
    "    pre_github_request_checker()\n",
    "    # get file list at root path\n",
    "    contents = repo.get_contents(\"/\")\n",
    "    for content in contents:\n",
    "        download_content(repo,content,extensions,out_dir)\n",
    "    df = pd.DataFrame.from_dict(files_dict,orient='index')\n",
    "    print(f\"Saving csv index with {len(df)} entries\")\n",
    "    df.to_csv(os.path.join(out_dir,\"index.csv\"))\n",
    "    \n",
    "def download_all_repos(df,extensions,out_dir):\n",
    "    all_repo_ids = list(df['id'])\n",
    "    for i in range(0,len(df['id'])):\n",
    "        repo_id = all_repo_ids[i]\n",
    "        pre_github_request_checker()\n",
    "        repo = g.get_repo(repo_id)\n",
    "        repo_dir = os.path.join(out_dir,str(repo_id))\n",
    "        if not os.path.exists(repo_dir):\n",
    "            os.makedirs(repo_dir)\n",
    "        print(f\"Searching repo {i} with id: {repo_id}\")\n",
    "        download_files_from_repo(repo,extensions,repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.rate_limiting\n",
    "# repo = g.get_repo(279998)\n",
    "# contents = repo.get_contents(\"/\")\n",
    "# print(contents)\n",
    "# contents[0].raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'verilog_extension_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m download_all_repos(systemverilog_df, \u001b[43mverilog_extension_files\u001b[49m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrelpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/repos\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'verilog_extension_files' is not defined"
     ]
    }
   ],
   "source": [
    "download_all_repos(systemverilog_df, verilog_extension_files, \"./data/full_repos/raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index file\n",
    "\n",
    "This step collects all files into data samples, each file is a code file, which is followed by the dataset splitting step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_df(downloaded_repo_dir,extensions_to_keep):\n",
    "    extensions_map = {ext: True for ext in extensions_to_keep}\n",
    "    df = pd.DataFrame(columns=['directory','repo_id','file_name','extension'])\n",
    "    for repo_id in os.listdir(downloaded_repo_dir):\n",
    "        for root,dirs,files in os.walk(os.path.join(downloaded_repo_dir,repo_id)):\n",
    "            for file in files:\n",
    "                extension = get_file_extension(file)\n",
    "                try:\n",
    "                    if extensions_map[extension]:\n",
    "                        directory = os.path.join(root,file)\n",
    "                        df.loc[len(df)] = [directory, repo_id, file, extension]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    extensions_map[extension] = False\n",
    "        print(f\"Done with repo: {repo_id}\")\n",
    "    print(f\"Extensions: {extensions_map}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verilog_files_df = create_files_df('data/full_repos/permissive',verilog_extension_files + system_verilog_extension_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "verilog_files_df.to_csv('./files_index.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition dataset for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_index = pd.read_csv('data/search_repo_indices/files_index.csv',index_col=0)\n",
    "# files_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314877"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_index = files_index[files_index['extension'].isin(extensions_to_keep)]\n",
    "# files_index = files_index.reset_index(drop=True)\n",
    "len(files_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_index = files_index.reset_index(drop=True)\n",
    "# files_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_indices = np.random.choice(len(files_index),replace=False,size=200)\n",
    "remaining_files_index = files_index.drop(index=few_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314677"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_files_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_partitions = 10\n",
    "tot_len = len(remaining_files_index)\n",
    "for i in range(number_of_partitions):\n",
    "    partition_df = remaining_files_index.iloc[list(range(i*tot_len//number_of_partitions,(i+1)*tot_len//number_of_partitions))]\n",
    "    partition_df.to_csv(f\"data/verilog_partitions/files_index_part_{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_len = len(remaining_files_index)\n",
    "total = 0\n",
    "for i in range(number_of_partitions):\n",
    "    length = len(pd.read_csv(f\"data/verilog_partitions/files_index_part_{i}.csv\"))\n",
    "    total += length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill partitions with source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_source_code(directory):\n",
    "    # Biggest error = utf-8 encoding problem\n",
    "    try:\n",
    "        # return open(directory,'r').read()\n",
    "        with codecs.open(directory,encoding='utf-8', errors='replace', mode = 'r') as f:\n",
    "            data = f.read()\n",
    "        return data.replace(\"\\x00\",\"\") # replacing this might not be needed but someone online said it helps...\n",
    "    except Exception as e:\n",
    "        e_string = f\"0:FOUND ERROR: {e}\"\n",
    "        print(e_string)\n",
    "        return e_string\n",
    "\n",
    "def clean_row_directory(row):\n",
    "    return row['directory'].replace(\"\\\\\",\"/\")\n",
    "\n",
    "def add_source_code_to_index_df(df):\n",
    "    df['directory'] = df.apply(lambda row: clean_row_directory(row),axis=1)\n",
    "    df['code'] = \"\"\n",
    "    tqdm.pandas(desc='Apply read_source_code')\n",
    "    df['code'] = df.progress_apply(lambda row: read_source_code(row['directory']),axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Apply read_source_code:  94%|█████████▎| 29496/31468 [05:13<00:28, 70.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Apply read_source_code: 100%|██████████| 31468/31468 [05:48<00:00, 90.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "number_of_partitions = 10\n",
    "for i in range(number_of_partitions):\n",
    "    df_dir = f\"data/verilog_partitions/files_index_part_{i}.csv\"\n",
    "    print(f\"Starting {i}\")\n",
    "    partition_df = pd.read_csv(df_dir,index_col=0)\n",
    "    new_partition_df = add_source_code_to_index_df(partition_df)\n",
    "    new_partition_df.to_csv(df_dir)\n",
    "    del partition_df, new_partition_df\n",
    "print(\"All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('hdl_dataset_creation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d483179aadbe36b266083fb168142eacd02134ef8f8b2756794bec1efb632f92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
