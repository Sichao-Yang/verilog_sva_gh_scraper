{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "\n",
    "1. search for 'verilog' repos;\n",
    "2. keep repo with permissive license;\n",
    "3. download repo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 在python脚本里设置代理\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from github import Github\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import  os\n",
    "ACCESS_TOKEN = os.getenv('git_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'limit': 5000, 'used': 0, 'remaining': 5000, 'reset': 1712896820}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_rate_limit().core.raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stall requesting when reaching the limit (5000 requests per hour)\n",
    "def pre_github_request_checker():\n",
    "    rate_data = g.get_rate_limit().core.raw_data\n",
    "    if rate_data['remaining'] < 50:\n",
    "        time_to_reset = rate_data['reset'] - int(time.time()) + 1\n",
    "        print(f\"Sleeping for {time_to_reset} seconds\")\n",
    "        time.sleep(time_to_reset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_github(language, start_date, end_date, permissible_lang_list=['Verilog', 'SystemVerilog']):\n",
    "    \"\"\"\n",
    "    More info:\n",
    "    https://docs.github.com/en/search-github/getting-started-with-searching-on-github/understanding-the-search-syntax#query-for-dates\n",
    "    \"\"\"\n",
    "    assert language in permissible_lang_list\n",
    "    date_q = f\"{start_date.strftime('%Y-%m-%d')}..{end_date.strftime('%Y-%m-%d')}\" \n",
    "    result = g.search_repositories(\"\",language=language, created=date_q) \n",
    "    print(f\"Found {result.totalCount} repos for: {language}, {date_q}\")\n",
    "    return result\n",
    "\n",
    "# results = search_github('Verilog', datetime(1980,1,1), datetime(2010,1,1))\n",
    "# results = search_github('Verilog', datetime(1980,1,1), datetime(2015,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repo_to_df(df, repo, data_field):\n",
    "    data = [getattr(repo, attr) for attr in data_field]\n",
    "    data.append(repo.get_topics())\n",
    "    try:\n",
    "        license_url = repo.get_license().license.url\n",
    "    except:\n",
    "        license_url = \"None\"\n",
    "    data.append(license_url)\n",
    "    df.loc[len(df)] = data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get repo info, add to df\n",
    "def process_repo_search_results(df, results, data_field):\n",
    "    for i in range(1000):\n",
    "        rate_data = g.get_rate_limit().core.raw_data\n",
    "        now_seconds = int(time.time())\n",
    "        if rate_data['remaining'] < 100:\n",
    "            time_to_reset = rate_data['reset'] - int(time.time()) + 1\n",
    "            print(f\"Sleeping for {time_to_reset} seconds\")\n",
    "            time.sleep(time_to_reset)\n",
    "        page = results.get_page(i)\n",
    "        page_size = len(page)\n",
    "        for j in range(page_size):\n",
    "            df = add_repo_to_df(df, page[j], data_field)\n",
    "        if page_size < 30:\n",
    "            break   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path as osp\n",
    "\n",
    "def init_df(data_field):\n",
    "    return pd.DataFrame(columns=data_field + [\"topics\",\"license_url\"])\n",
    "\n",
    "def make_datetime(string, format='%Y-%m-%d'):\n",
    "    return datetime.strptime(string, format)\n",
    "\n",
    "def str_datetime(dt, format='%Y-%m-%d'):\n",
    "    return dt.strftime(format)\n",
    "\n",
    "def append_msg(msg, path):\n",
    "    with open(path, 'a') as fp:\n",
    "        fp.write(msg+'\\n')\n",
    "\n",
    "def find_repos(language, date_list, data_field, root_path='data/repo_index', search_limit=1000):\n",
    "    os.makedirs(root_path, exist_ok=True)\n",
    "    log_path = osp.join(root_path, 'log')\n",
    "    \n",
    "    while len(date_list)>0:\n",
    "        start_date, end_date = date_list.pop()\n",
    "        # search by lang, start & end dates\n",
    "        repo_search_results = search_github(language, start_date, end_date)\n",
    "        if repo_search_results.totalCount > 0:\n",
    "            if repo_search_results.totalCount >= search_limit:\n",
    "                # Reduce date range recursively\n",
    "                delta = (end_date - start_date) / 2\n",
    "                date_list.append((start_date + delta, end_date))\n",
    "                date_list.append((start_date, end_date - delta))\n",
    "                print(f\"Search count > {search_limit} ({repo_search_results.totalCount}) in ({start_date}, {end_date}), split to {date_list[-1]} and {date_list[-2]}\")\n",
    "            else:\n",
    "                df = init_df(data_field)\n",
    "                process_repo_search_results(df, repo_search_results, data_field)\n",
    "                file_name = f\"{language}_{str_datetime(start_date)}_{str_datetime(end_date)}.csv\"\n",
    "                df.to_csv(osp.join(root_path, file_name))\n",
    "                msg = f\"Done: {str_datetime(start_date)}..{str_datetime(end_date)}, length: {len(df)}\"\n",
    "                append_msg(msg, log_path)\n",
    "                print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_field = [\n",
    "    \"id\",\n",
    "    \"clone_url\",\n",
    "    \"created_at\",\n",
    "    \"description\",\n",
    "    \"full_name\",\n",
    "    \"language\",\n",
    "    \"name\",\n",
    "    \"size\",\n",
    "    \"stargazers_count\",\n",
    "    \"updated_at\",\n",
    "    \"forks_count\"\n",
    "]\n",
    "data_field.sort()\n",
    "language = \"Verilog\"\n",
    "\n",
    "# find_repos(language, [(datetime(2010,1,1), datetime(2010,5,1))], data_field, search_limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add date_range to task list\n",
    "\n",
    "since github api's max rate limit per hour is 1000, so we need to split the search date range. you can either:\n",
    "\n",
    "1. set the whole range and let the function find_repo to do the binary split for you;\n",
    "2. or split manually (likely based on previous search experience) and pass the date_list to find_repo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. set full range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2000,1,1)\n",
    "end_date = datetime(2023,11,7)\n",
    "date_list = [(start_date, end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. set list of ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_num_date(text):\n",
    "    # Extracting number and dates using one pattern\n",
    "    pattern = r'Found (\\d+) repos for: \\w+, (\\d{4}-\\d{2}-\\d{2})\\.\\.(\\d{4}-\\d{2}-\\d{2})'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        number = match.group(1)\n",
    "        start_date = match.group(2)\n",
    "        end_date = match.group(3)\n",
    "        # print(\"Number:\", number)\n",
    "        # print(\"Start Date:\", start_date)\n",
    "        # print(\"End Date:\", end_date)\n",
    "        return (number, (start_date, end_date))\n",
    "    else:\n",
    "        # print(\"No match found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [clone_url, created_at, description, forks_count, full_name, id, language, name, size, stargazers_count, updated_at, topics, license_url]\n",
      "Index: []\n",
      "Found 1000 repos for: Verilog, 2000-01-01..2024-04-10\n",
      "Found 279 repos for: Verilog, 2000-01-01..2012-02-20\n",
      "Done: 2000-01-01..2012-02-20, df length: 279\n",
      "Found 1000 repos for: Verilog, 2012-02-20..2024-04-10\n",
      "Found 1000 repos for: Verilog, 2012-02-20..2018-03-17\n",
      "Found 1000 repos for: Verilog, 2012-02-20..2015-03-04\n",
      "Found 643 repos for: Verilog, 2012-02-20..2013-08-27\n"
     ]
    }
   ],
   "source": [
    "with open('github.log') as fp:\n",
    "    log = fp.readlines()\n",
    "for i in range(10):\n",
    "    print(log[i], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2014, 10, 17, 0, 0), datetime.datetime(2015, 3, 4, 0, 0)),\n",
      " (datetime.datetime(2014, 5, 31, 0, 0), datetime.datetime(2014, 10, 17, 0, 0)),\n",
      " (datetime.datetime(2013, 8, 27, 0, 0), datetime.datetime(2014, 5, 31, 0, 0)),\n",
      " (datetime.datetime(2012, 2, 20, 0, 0), datetime.datetime(2013, 8, 27, 0, 0)),\n",
      " (datetime.datetime(2000, 1, 1, 0, 0), datetime.datetime(2012, 2, 20, 0, 0))]\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "date_list = []\n",
    "for l in log:\n",
    "    ret = find_num_date(l)\n",
    "    if ret is not None:\n",
    "        if eval(ret[0]) != 1000:\n",
    "            date_list.append(\n",
    "                (make_datetime(ret[1][0]), make_datetime(ret[1][1]))\n",
    "                )\n",
    "            \n",
    "# reverse the list and pop from back\n",
    "date_list = date_list[::-1]\n",
    "pprint(date_list[-5:])\n",
    "print(len(date_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 repos for: Verilog, 2023-11-07..2024-04-13\n",
      "Search count > 1000 (1000) in (2023-11-07 00:00:00, 2024-04-13 19:46:51.460603), split to (datetime.datetime(2024, 1, 25, 9, 53, 25, 730302), datetime.datetime(2024, 4, 13, 19, 46, 51, 460603)) and (datetime.datetime(2023, 11, 7, 0, 0), datetime.datetime(2024, 1, 25, 9, 53, 25, 730301))\n",
      "Found 1000 repos for: Verilog, 2024-01-25..2024-04-13\n",
      "Search count > 1000 (1000) in (2024-01-25 09:53:25.730302, 2024-04-13 19:46:51.460603), split to (datetime.datetime(2024, 3, 5, 2, 50, 8, 595452), datetime.datetime(2024, 4, 13, 19, 46, 51, 460603)) and (datetime.datetime(2024, 1, 25, 9, 53, 25, 730302), datetime.datetime(2024, 3, 5, 2, 50, 8, 595453))\n",
      "Found 1000 repos for: Verilog, 2024-03-05..2024-04-13\n",
      "Search count > 1000 (1000) in (2024-03-05 02:50:08.595452, 2024-04-13 19:46:51.460603), split to (datetime.datetime(2024, 3, 24, 23, 18, 30, 28028), datetime.datetime(2024, 4, 13, 19, 46, 51, 460603)) and (datetime.datetime(2024, 3, 5, 2, 50, 8, 595452), datetime.datetime(2024, 3, 24, 23, 18, 30, 28027))\n",
      "Found 937 repos for: Verilog, 2024-03-24..2024-04-13\n",
      "Done: 2024-03-24..2024-04-13, length: 937\n",
      "Found 1000 repos for: Verilog, 2024-03-05..2024-03-24\n",
      "Search count > 1000 (1000) in (2024-03-05 02:50:08.595452, 2024-03-24 23:18:30.028027), split to (datetime.datetime(2024, 3, 15, 1, 4, 19, 311740), datetime.datetime(2024, 3, 24, 23, 18, 30, 28027)) and (datetime.datetime(2024, 3, 5, 2, 50, 8, 595452), datetime.datetime(2024, 3, 15, 1, 4, 19, 311739))\n",
      "Found 505 repos for: Verilog, 2024-03-15..2024-03-24\n",
      "Done: 2024-03-15..2024-03-24, length: 505\n",
      "Found 579 repos for: Verilog, 2024-03-05..2024-03-15\n",
      "Done: 2024-03-05..2024-03-15, length: 579\n",
      "Found 1000 repos for: Verilog, 2024-01-25..2024-03-05\n",
      "Search count > 1000 (1000) in (2024-01-25 09:53:25.730302, 2024-03-05 02:50:08.595453), split to (datetime.datetime(2024, 2, 14, 6, 21, 47, 162878), datetime.datetime(2024, 3, 5, 2, 50, 8, 595453)) and (datetime.datetime(2024, 1, 25, 9, 53, 25, 730302), datetime.datetime(2024, 2, 14, 6, 21, 47, 162877))\n",
      "Found 907 repos for: Verilog, 2024-02-14..2024-03-05\n",
      "Sleeping for 142 seconds\n",
      "Done: 2024-02-14..2024-03-05, length: 907\n",
      "Found 792 repos for: Verilog, 2024-01-25..2024-02-14\n",
      "Done: 2024-01-25..2024-02-14, length: 791\n",
      "Found 1000 repos for: Verilog, 2023-11-07..2024-01-25\n",
      "Search count > 1000 (1000) in (2023-11-07 00:00:00, 2024-01-25 09:53:25.730301), split to (datetime.datetime(2023, 12, 16, 16, 56, 42, 865150), datetime.datetime(2024, 1, 25, 9, 53, 25, 730301)) and (datetime.datetime(2023, 11, 7, 0, 0), datetime.datetime(2023, 12, 16, 16, 56, 42, 865151))\n",
      "Found 1000 repos for: Verilog, 2023-12-16..2024-01-25\n",
      "Search count > 1000 (1000) in (2023-12-16 16:56:42.865150, 2024-01-25 09:53:25.730301), split to (datetime.datetime(2024, 1, 5, 13, 25, 4, 297726), datetime.datetime(2024, 1, 25, 9, 53, 25, 730301)) and (datetime.datetime(2023, 12, 16, 16, 56, 42, 865150), datetime.datetime(2024, 1, 5, 13, 25, 4, 297725))\n",
      "Found 923 repos for: Verilog, 2024-01-05..2024-01-25\n",
      "Done: 2024-01-05..2024-01-25, length: 923\n",
      "Found 820 repos for: Verilog, 2023-12-16..2024-01-05\n",
      "Sleeping for 97 seconds\n",
      "Done: 2023-12-16..2024-01-05, length: 820\n",
      "Found 1000 repos for: Verilog, 2023-11-07..2023-12-16\n",
      "Search count > 1000 (1000) in (2023-11-07 00:00:00, 2023-12-16 16:56:42.865151), split to (datetime.datetime(2023, 11, 26, 20, 28, 21, 432576), datetime.datetime(2023, 12, 16, 16, 56, 42, 865151)) and (datetime.datetime(2023, 11, 7, 0, 0), datetime.datetime(2023, 11, 26, 20, 28, 21, 432575))\n",
      "Found 989 repos for: Verilog, 2023-11-26..2023-12-16\n",
      "Done: 2023-11-26..2023-12-16, length: 989\n",
      "Found 969 repos for: Verilog, 2023-11-07..2023-11-26\n",
      "Sleeping for 133 seconds\n",
      "Done: 2023-11-07..2023-11-26, length: 969\n"
     ]
    }
   ],
   "source": [
    "find_repos(language, date_list, data_field, root_path='data/repo_index', search_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 repos for: SystemVerilog, 2000-01-01..2023-11-07\n",
      "Search count > 1000 (1000) in (2000-01-01 00:00:00, 2023-11-07 00:00:00), split to (datetime.datetime(2011, 12, 4, 12, 0), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2000, 1, 1, 0, 0), datetime.datetime(2011, 12, 4, 12, 0))\n",
      "Found 1000 repos for: SystemVerilog, 2011-12-04..2023-11-07\n",
      "Search count > 1000 (1000) in (2011-12-04 12:00:00, 2023-11-07 00:00:00), split to (datetime.datetime(2017, 11, 20, 6, 0), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2011, 12, 4, 12, 0), datetime.datetime(2017, 11, 20, 6, 0))\n",
      "Found 1000 repos for: SystemVerilog, 2017-11-20..2023-11-07\n",
      "Search count > 1000 (1000) in (2017-11-20 06:00:00, 2023-11-07 00:00:00), split to (datetime.datetime(2020, 11, 13, 3, 0), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2017, 11, 20, 6, 0), datetime.datetime(2020, 11, 13, 3, 0))\n",
      "Found 1000 repos for: SystemVerilog, 2020-11-13..2023-11-07\n",
      "Search count > 1000 (1000) in (2020-11-13 03:00:00, 2023-11-07 00:00:00), split to (datetime.datetime(2022, 5, 11, 13, 30), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2020, 11, 13, 3, 0), datetime.datetime(2022, 5, 11, 13, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2022-05-11..2023-11-07\n",
      "Search count > 1000 (1000) in (2022-05-11 13:30:00, 2023-11-07 00:00:00), split to (datetime.datetime(2023, 2, 7, 18, 45), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2022, 5, 11, 13, 30), datetime.datetime(2023, 2, 7, 18, 45))\n",
      "Found 1000 repos for: SystemVerilog, 2023-02-07..2023-11-07\n",
      "Search count > 1000 (1000) in (2023-02-07 18:45:00, 2023-11-07 00:00:00), split to (datetime.datetime(2023, 6, 23, 21, 22, 30), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2023, 2, 7, 18, 45), datetime.datetime(2023, 6, 23, 21, 22, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2023-06-23..2023-11-07\n",
      "Search count > 1000 (1000) in (2023-06-23 21:22:30, 2023-11-07 00:00:00), split to (datetime.datetime(2023, 8, 30, 22, 41, 15), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2023, 6, 23, 21, 22, 30), datetime.datetime(2023, 8, 30, 22, 41, 15))\n",
      "Found 1000 repos for: SystemVerilog, 2023-08-30..2023-11-07\n",
      "Search count > 1000 (1000) in (2023-08-30 22:41:15, 2023-11-07 00:00:00), split to (datetime.datetime(2023, 10, 3, 23, 20, 37, 500000), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2023, 8, 30, 22, 41, 15), datetime.datetime(2023, 10, 3, 23, 20, 37, 500000))\n",
      "Found 502 repos for: SystemVerilog, 2023-10-03..2023-11-07\n",
      "Sleeping for 68 seconds\n",
      "Done: 2023-10-03..2023-11-07, length: 502\n",
      "Found 514 repos for: SystemVerilog, 2023-08-30..2023-10-03\n",
      "Done: 2023-08-30..2023-10-03, length: 514\n",
      "Found 752 repos for: SystemVerilog, 2023-06-23..2023-08-30\n",
      "Done: 2023-06-23..2023-08-30, length: 752\n",
      "Found 1000 repos for: SystemVerilog, 2023-02-07..2023-06-23\n",
      "Search count > 1000 (1000) in (2023-02-07 18:45:00, 2023-06-23 21:22:30), split to (datetime.datetime(2023, 4, 16, 20, 3, 45), datetime.datetime(2023, 6, 23, 21, 22, 30)) and (datetime.datetime(2023, 2, 7, 18, 45), datetime.datetime(2023, 4, 16, 20, 3, 45))\n",
      "Found 743 repos for: SystemVerilog, 2023-04-16..2023-06-23\n",
      "Done: 2023-04-16..2023-06-23, length: 743\n",
      "Found 816 repos for: SystemVerilog, 2023-02-07..2023-04-16\n",
      "Sleeping for 155 seconds\n",
      "Done: 2023-02-07..2023-04-16, length: 816\n",
      "Found 1000 repos for: SystemVerilog, 2022-05-11..2023-02-07\n",
      "Search count > 1000 (1000) in (2022-05-11 13:30:00, 2023-02-07 18:45:00), split to (datetime.datetime(2022, 9, 24, 16, 7, 30), datetime.datetime(2023, 2, 7, 18, 45)) and (datetime.datetime(2022, 5, 11, 13, 30), datetime.datetime(2022, 9, 24, 16, 7, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2022-09-24..2023-02-07\n",
      "Search count > 1000 (1000) in (2022-09-24 16:07:30, 2023-02-07 18:45:00), split to (datetime.datetime(2022, 12, 1, 17, 26, 15), datetime.datetime(2023, 2, 7, 18, 45)) and (datetime.datetime(2022, 9, 24, 16, 7, 30), datetime.datetime(2022, 12, 1, 17, 26, 15))\n",
      "Found 572 repos for: SystemVerilog, 2022-12-01..2023-02-07\n",
      "Done: 2022-12-01..2023-02-07, length: 572\n",
      "Found 621 repos for: SystemVerilog, 2022-09-24..2022-12-01\n",
      "Done: 2022-09-24..2022-12-01, length: 621\n",
      "Found 998 repos for: SystemVerilog, 2022-05-11..2022-09-24\n",
      "Sleeping for 152 seconds\n",
      "Done: 2022-05-11..2022-09-24, length: 998\n",
      "Found 1000 repos for: SystemVerilog, 2020-11-13..2022-05-11\n",
      "Search count > 1000 (1000) in (2020-11-13 03:00:00, 2022-05-11 13:30:00), split to (datetime.datetime(2021, 8, 12, 8, 15), datetime.datetime(2022, 5, 11, 13, 30)) and (datetime.datetime(2020, 11, 13, 3, 0), datetime.datetime(2021, 8, 12, 8, 15))\n",
      "Found 1000 repos for: SystemVerilog, 2021-08-12..2022-05-11\n",
      "Search count > 1000 (1000) in (2021-08-12 08:15:00, 2022-05-11 13:30:00), split to (datetime.datetime(2021, 12, 26, 10, 52, 30), datetime.datetime(2022, 5, 11, 13, 30)) and (datetime.datetime(2021, 8, 12, 8, 15), datetime.datetime(2021, 12, 26, 10, 52, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2021-12-26..2022-05-11\n",
      "Search count > 1000 (1000) in (2021-12-26 10:52:30, 2022-05-11 13:30:00), split to (datetime.datetime(2022, 3, 4, 12, 11, 15), datetime.datetime(2022, 5, 11, 13, 30)) and (datetime.datetime(2021, 12, 26, 10, 52, 30), datetime.datetime(2022, 3, 4, 12, 11, 15))\n",
      "Found 620 repos for: SystemVerilog, 2022-03-04..2022-05-11\n",
      "Done: 2022-03-04..2022-05-11, length: 620\n",
      "Found 434 repos for: SystemVerilog, 2021-12-26..2022-03-04\n",
      "Done: 2021-12-26..2022-03-04, length: 434\n",
      "Found 873 repos for: SystemVerilog, 2021-08-12..2021-12-26\n",
      "Done: 2021-08-12..2021-12-26, length: 873\n",
      "Found 1000 repos for: SystemVerilog, 2020-11-13..2021-08-12\n",
      "Search count > 1000 (1000) in (2020-11-13 03:00:00, 2021-08-12 08:15:00), split to (datetime.datetime(2021, 3, 29, 5, 37, 30), datetime.datetime(2021, 8, 12, 8, 15)) and (datetime.datetime(2020, 11, 13, 3, 0), datetime.datetime(2021, 3, 29, 5, 37, 30))\n",
      "Found 822 repos for: SystemVerilog, 2021-03-29..2021-08-12\n",
      "Sleeping for 109 seconds\n",
      "Done: 2021-03-29..2021-08-12, length: 822\n",
      "Found 758 repos for: SystemVerilog, 2020-11-13..2021-03-29\n",
      "Done: 2020-11-13..2021-03-29, length: 758\n",
      "Found 1000 repos for: SystemVerilog, 2017-11-20..2020-11-13\n",
      "Search count > 1000 (1000) in (2017-11-20 06:00:00, 2020-11-13 03:00:00), split to (datetime.datetime(2019, 5, 18, 16, 30), datetime.datetime(2020, 11, 13, 3, 0)) and (datetime.datetime(2017, 11, 20, 6, 0), datetime.datetime(2019, 5, 18, 16, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2019-05-18..2020-11-13\n",
      "Search count > 1000 (1000) in (2019-05-18 16:30:00, 2020-11-13 03:00:00), split to (datetime.datetime(2020, 2, 14, 21, 45), datetime.datetime(2020, 11, 13, 3, 0)) and (datetime.datetime(2019, 5, 18, 16, 30), datetime.datetime(2020, 2, 14, 21, 45))\n",
      "Found 1000 repos for: SystemVerilog, 2020-02-14..2020-11-13\n",
      "Search count > 1000 (1000) in (2020-02-14 21:45:00, 2020-11-13 03:00:00), split to (datetime.datetime(2020, 6, 30, 0, 22, 30), datetime.datetime(2020, 11, 13, 3, 0)) and (datetime.datetime(2020, 2, 14, 21, 45), datetime.datetime(2020, 6, 30, 0, 22, 30))\n",
      "Found 644 repos for: SystemVerilog, 2020-06-30..2020-11-13\n",
      "Done: 2020-06-30..2020-11-13, length: 644\n",
      "Found 662 repos for: SystemVerilog, 2020-02-14..2020-06-30\n",
      "Sleeping for 129 seconds\n",
      "Done: 2020-02-14..2020-06-30, length: 662\n",
      "Found 1000 repos for: SystemVerilog, 2019-05-18..2020-02-14\n",
      "Search count > 1000 (1000) in (2019-05-18 16:30:00, 2020-02-14 21:45:00), split to (datetime.datetime(2019, 10, 1, 19, 7, 30), datetime.datetime(2020, 2, 14, 21, 45)) and (datetime.datetime(2019, 5, 18, 16, 30), datetime.datetime(2019, 10, 1, 19, 7, 30))\n",
      "Found 563 repos for: SystemVerilog, 2019-10-01..2020-02-14\n",
      "Done: 2019-10-01..2020-02-14, length: 563\n",
      "Found 501 repos for: SystemVerilog, 2019-05-18..2019-10-01\n",
      "Done: 2019-05-18..2019-10-01, length: 501\n",
      "Found 1000 repos for: SystemVerilog, 2017-11-20..2019-05-18\n",
      "Search count > 1000 (1000) in (2017-11-20 06:00:00, 2019-05-18 16:30:00), split to (datetime.datetime(2018, 8, 19, 11, 15), datetime.datetime(2019, 5, 18, 16, 30)) and (datetime.datetime(2017, 11, 20, 6, 0), datetime.datetime(2018, 8, 19, 11, 15))\n",
      "Found 932 repos for: SystemVerilog, 2018-08-19..2019-05-18\n",
      "Done: 2018-08-19..2019-05-18, length: 932\n",
      "Found 689 repos for: SystemVerilog, 2017-11-20..2018-08-19\n",
      "Sleeping for 98 seconds\n",
      "Done: 2017-11-20..2018-08-19, length: 689\n",
      "Found 1000 repos for: SystemVerilog, 2011-12-04..2017-11-20\n",
      "Search count > 1000 (1000) in (2011-12-04 12:00:00, 2017-11-20 06:00:00), split to (datetime.datetime(2014, 11, 27, 9, 0), datetime.datetime(2017, 11, 20, 6, 0)) and (datetime.datetime(2011, 12, 4, 12, 0), datetime.datetime(2014, 11, 27, 9, 0))\n",
      "Found 1000 repos for: SystemVerilog, 2014-11-27..2017-11-20\n",
      "Search count > 1000 (1000) in (2014-11-27 09:00:00, 2017-11-20 06:00:00), split to (datetime.datetime(2016, 5, 24, 19, 30), datetime.datetime(2017, 11, 20, 6, 0)) and (datetime.datetime(2014, 11, 27, 9, 0), datetime.datetime(2016, 5, 24, 19, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2016-05-24..2017-11-20\n",
      "Search count > 1000 (1000) in (2016-05-24 19:30:00, 2017-11-20 06:00:00), split to (datetime.datetime(2017, 2, 21, 0, 45), datetime.datetime(2017, 11, 20, 6, 0)) and (datetime.datetime(2016, 5, 24, 19, 30), datetime.datetime(2017, 2, 21, 0, 45))\n",
      "Found 557 repos for: SystemVerilog, 2017-02-21..2017-11-20\n",
      "Done: 2017-02-21..2017-11-20, length: 557\n",
      "Found 446 repos for: SystemVerilog, 2016-05-24..2017-02-21\n",
      "Done: 2016-05-24..2017-02-21, length: 446\n",
      "Found 487 repos for: SystemVerilog, 2014-11-27..2016-05-24\n",
      "Done: 2014-11-27..2016-05-24, length: 487\n",
      "Found 141 repos for: SystemVerilog, 2011-12-04..2014-11-27\n",
      "Done: 2011-12-04..2014-11-27, length: 141\n",
      "Found 4 repos for: SystemVerilog, 2000-01-01..2011-12-04\n",
      "Done: 2000-01-01..2011-12-04, length: 4\n"
     ]
    }
   ],
   "source": [
    "language = 'SystemVerilog'\n",
    "find_repos(language, date_list, data_field, root_path='data/repo_index', search_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(f\"data/{language}_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concate repo dfs and deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_deduplicate_gh_search_results(csvs):\n",
    "    df = pd.concat(map(lambda x: pd.read_csv(x, na_values=['None']), csvs), ignore_index=True)\n",
    "    df = df.drop(['Unnamed: 0'],axis=1)\n",
    "    df = df.drop_duplicates([c for c in df.columns if c != 'updated_at'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from os import path as osp\n",
    "import os\n",
    "\n",
    "v_filepaths = ['./Verilog_1980-01-01_2012-01-01.csv']\n",
    "sv_filepaths = ['./SystemVerilog_1980-01-01_2012-01-01.csv']\n",
    "\n",
    "verilog_df = combine_and_deduplicate_gh_search_results(v_filepaths)\n",
    "systemverilog_df = combine_and_deduplicate_gh_search_results(sv_filepaths)\n",
    "\n",
    "print(len(verilog_df))\n",
    "print(len(systemverilog_df))\n",
    "\n",
    "repo_indices_dir = 'data/search_repo_indices'\n",
    "os.makedirs(repo_indices_dir)\n",
    "verilog_df.to_csv(osp.join(repo_indices_dir,\"full_verilog_repos.csv\"))\n",
    "systemverilog_df.to_csv(osp.join(repo_indices_dir,\"full_systemverilog_repos.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([verilog_df,systemverilog_df]).drop_duplicates(subset=[c for c in verilog_df.columns if not c in ['language']])\n",
    "all_df.to_csv(\"data/all_deduplicated_repos.csv\")\n",
    "print(len(all_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define extensions to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.intel.com/content/www/us/en/programmable/quartushelp/17.0/reference/glossary/glosslist.htm\n",
    "# https://marketplace.visualstudio.com/items?itemName=eirikpre.systemverilog\n",
    "verilog_extension_files = ['v','verilog','vlg','vh']\n",
    "system_verilog_extension_files = ['sv','svh','svp', 'sva']\n",
    "extra_file_types = ['vo','vt'] # verilog output, verilog test bench\n",
    "\n",
    "extensions_to_keep = verilog_extension_files + system_verilog_extension_files + extra_file_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding licenses and remove non permissive repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_licenses_from_repo_df_to_dict(repo_df, licenses_dict):\n",
    "    df_with_unique_licenses = repo_df.loc[repo_df['license_url'].dropna().drop_duplicates().index]\n",
    "    repo_ids = list(df_with_unique_licenses['id'])\n",
    "    for rid in repo_ids:\n",
    "        pre_github_request_checker()\n",
    "        license_data = g.get_repo(rid).get_license().license.raw_data\n",
    "        licenses_dict[license_data['url']] = license_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "licenses_dict = {}\n",
    "add_licenses_from_repo_df_to_dict(verilog_df,licenses_dict)\n",
    "add_licenses_from_repo_df_to_dict(systemverilog_df,licenses_dict)\n",
    "df = pd.DataFrame.from_dict(licenses_dict, orient='index')\n",
    "df.to_csv(os.path.join('data/search_repo_indices','licenses.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_licenses_with_permissions_conditions(license_df,permissions=[],conditions=[]):\n",
    "    indices = []\n",
    "    for i, row in license_df.iterrows():\n",
    "        if len(permissions) == 0 or set(permissions).issubset(set(row['permissions'])):\n",
    "            if len(conditions) == 0 or len(set(conditions).intersection(set(row['conditions']))) > 0 :\n",
    "                indices.append(i)\n",
    "    return license_df.loc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "permissions = ['modifications','distribution']\n",
    "special_conditions = ['same-license--file','same-license--library','same-license']\n",
    "permissive_licenses_df = get_licenses_with_permissions_conditions(df,permissions=permissions,conditions=[])\n",
    "distributive_licenses_df = get_licenses_with_permissions_conditions(df,permissions=['distribution'],conditions=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(permissive_licenses_df))\n",
    "print(len(distributive_licenses_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion, all licenses found are permissive in that they allow modifications and distribution! Repos without licenses are not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sv_df = systemverilog_df.dropna(subset=['license_url'])\n",
    "p_ve_df = verilog_df.dropna(subset=['license_url'])\n",
    "\n",
    "p_sv_df.to_csv(os.path.join(repo_indices_dir,\"permissive_systemverilog_repos.csv\"))\n",
    "p_ve_df.to_csv(os.path.join(repo_indices_dir,\"permissive_verilog_repos.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_all_df = pd.concat([p_ve_df,p_sv_df]).drop_duplicates(subset=[c for c in verilog_df.columns if not c in ['language']])\n",
    "p_all_df.to_csv(\"data/permissive_all_deduplicated_repos.csv\")\n",
    "print(len(p_all_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)\n",
    "all_permissions = []\n",
    "all_conditions = []\n",
    "all_limitations = []\n",
    "for i,row in df.iterrows():\n",
    "    all_permissions.append(row['permissions'])\n",
    "    all_conditions.append(row['conditions'])\n",
    "    all_limitations.append(row['limitations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['include-copyright', 'document-changes', 'disclose-source', 'same-license'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['include-copyright', 'document-changes', 'disclose-source', 'same-license'],\n",
       " ['include-copyright'],\n",
       " ['include-copyright'],\n",
       " ['include-copyright',\n",
       "  'disclose-source',\n",
       "  'document-changes',\n",
       "  'same-license--library'],\n",
       " ['include-copyright',\n",
       "  'disclose-source',\n",
       "  'document-changes',\n",
       "  'same-license--library'],\n",
       " [],\n",
       " ['include-copyright'],\n",
       " ['include-copyright',\n",
       "  'document-changes',\n",
       "  'disclose-source',\n",
       "  'network-use-disclose',\n",
       "  'same-license'],\n",
       " [],\n",
       " ['include-copyright'],\n",
       " [],\n",
       " ['disclose-source', 'include-copyright', 'same-license--file'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['include-copyright', 'document-changes', 'same-license'],\n",
       " ['disclose-source', 'include-copyright', 'same-license'],\n",
       " ['include-copyright--source'],\n",
       " ['include-copyright--source', 'document-changes'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['disclose-source', 'include-copyright', 'same-license'],\n",
       " ['include-copyright']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conditions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download repos:\n",
    "\n",
    "There are two ways:\n",
    "1. Clone or download full repo, then filter out files;\n",
    "2. get into repos and download single files if its extension is the target ones.\n",
    "\n",
    "The first method requires more storage space and network usage, but the second method requires more github api requests (for each file, it requires one get_repo search + n get_contents search).\n",
    "\n",
    "therefore, the first method is prefered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. raw git clone\n",
    "\n",
    "1.1 create downloading bash script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'git clone --depth 1 --no-tags https://github.com/mrehkopf/sd2snes.git ./data/full_repos/exp'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_clone_command(clone_url, directory, depth=1, branch='master'):\n",
    "    command_parts = [f'git clone']\n",
    "    # TODO: rerun search without master branch (causes error for some searches!)\n",
    "    # if branch is not set, then the clone method will download default branch\n",
    "    # command_parts.append(f\"-b {branch}\") \n",
    "    command_parts.append(f\"--depth {depth}\")\n",
    "    command_parts.append(f\"--no-tags\")\n",
    "    # command_parts.append(f\"--no-checkout\")\n",
    "    command_parts.append(clone_url)\n",
    "    command_parts.append(directory)\n",
    "    return \" \".join(command_parts)\n",
    "\n",
    "def create_clone_script_for_df(repo_df,script_out_path,clone_out_dir):\n",
    "    with open(script_out_path,'w+') as f:\n",
    "        for i,row in repo_df.iterrows():\n",
    "            clone_url = row['clone_url']\n",
    "            out_dir = os.path.join(clone_out_dir,str(row['id']))\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.mkdir(out_dir)\n",
    "            f.write(create_clone_command(clone_url,str(os.path.abspath(out_dir))).replace(\"\\\\\",\"/\") + \"\\n\")\n",
    "\n",
    "create_clone_command(\"https://github.com/mrehkopf/sd2snes.git\",\"./data/full_repos/exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/full_repos/permissive\", exist_ok=True)\n",
    "create_clone_script_for_df(p_all_df, \"./data/clone_all_p.sh\",\"data/full_repos/permissive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash data/clone_all_p.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Filter repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument '1' in rsplit can limit the split to max 2 segments\n",
    "def get_file_extension(path):\n",
    "    return path.rsplit(\".\",1)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_files_without_right_extension(start_dir, extensions_to_keep):\n",
    "    errors = []\n",
    "    for root, dirs, files in os.walk(start_dir):\n",
    "        for file in [sf for sf in files if not get_file_extension(sf) in extensions_to_keep]:\n",
    "            file_path = os.path.join(root,file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "            except Exception as e:\n",
    "                errors.append(e)\n",
    "    return errors\n",
    "\n",
    "def delete_empty_dirs(start_dir):\n",
    "    errors = []\n",
    "    for root, dirs, files in os.walk(start_dir,topdown=False):\n",
    "        for d in dirs:\n",
    "            dir_path = os.path.join(root,d)\n",
    "            if len(os.listdir(dir_path)) == 0:\n",
    "                try:\n",
    "                    os.rmdir(dir_path)\n",
    "                except Exception as e:\n",
    "                    errors.append(e)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dir = \"data/full_repos/permissive\"\n",
    "\n",
    "files_errors = delete_all_files_without_right_extension(start_dir,extensions_to_keep)\n",
    "dirs_errors = delete_empty_dirs(start_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. fine-grained repo download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_files_dict(count, content):\n",
    "    global files_dict\n",
    "    files_dict[count] = {\n",
    "        \"path\": content.raw_data['path'],\n",
    "        \"size\": content.raw_data['size'],\n",
    "        \"count_id\": count\n",
    "    }\n",
    "\n",
    "def download_content(repo,content,extensions,out_dir):\n",
    "    content_raw_data = content.raw_data\n",
    "    content_type = content_raw_data['type']\n",
    "    if content_type == 'dir':\n",
    "        pre_github_request_checker()\n",
    "        new_contents = repo.get_contents(content_raw_data['path'])\n",
    "        for new_content in new_contents:\n",
    "            download_content(repo,new_content,extensions,out_dir)\n",
    "    elif content_type == 'file':\n",
    "        extension = get_file_extension(content_raw_data['name'])\n",
    "        if extension in extensions:\n",
    "            global file_count\n",
    "            update_files_dict(file_count,content)\n",
    "            pre_github_request_checker()\n",
    "            try:\n",
    "                with open(os.path.join(out_dir,str(file_count) + \".\" + extension),'wb') as f:\n",
    "                    f.write(content.decoded_content)\n",
    "                file_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Caught exception while trying to write content:\\n{e}\")\n",
    "    # raise Exception(f\"Content type not recognized: {content_type}\")\n",
    "\n",
    "def download_files_from_repo(repo,extensions,out_dir):\n",
    "    global files_dict\n",
    "    global file_count\n",
    "    file_count = 0\n",
    "    files_dict = {}\n",
    "    pre_github_request_checker()\n",
    "    # get file list at root path\n",
    "    contents = repo.get_contents(\"/\")\n",
    "    for content in contents:\n",
    "        download_content(repo,content,extensions,out_dir)\n",
    "    df = pd.DataFrame.from_dict(files_dict,orient='index')\n",
    "    print(f\"Saving csv index with {len(df)} entries\")\n",
    "    df.to_csv(os.path.join(out_dir,\"index.csv\"))\n",
    "    \n",
    "def download_all_repos(df,extensions,out_dir):\n",
    "    all_repo_ids = list(df['id'])\n",
    "    for i in range(0,len(df['id'])):\n",
    "        repo_id = all_repo_ids[i]\n",
    "        pre_github_request_checker()\n",
    "        repo = g.get_repo(repo_id)\n",
    "        repo_dir = os.path.join(out_dir,str(repo_id))\n",
    "        if not os.path.exists(repo_dir):\n",
    "            os.makedirs(repo_dir)\n",
    "        print(f\"Searching repo {i} with id: {repo_id}\")\n",
    "        download_files_from_repo(repo,extensions,repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.rate_limiting\n",
    "# repo = g.get_repo(279998)\n",
    "# contents = repo.get_contents(\"/\")\n",
    "# print(contents)\n",
    "# contents[0].raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'verilog_extension_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m download_all_repos(systemverilog_df, \u001b[43mverilog_extension_files\u001b[49m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrelpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/repos\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'verilog_extension_files' is not defined"
     ]
    }
   ],
   "source": [
    "download_all_repos(systemverilog_df, verilog_extension_files, \"./data/full_repos/raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index file\n",
    "\n",
    "This step collects all files into data samples, each file is a code file, which is followed by the dataset splitting step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_df(downloaded_repo_dir,extensions_to_keep):\n",
    "    extensions_map = {ext: True for ext in extensions_to_keep}\n",
    "    df = pd.DataFrame(columns=['directory','repo_id','file_name','extension'])\n",
    "    for repo_id in os.listdir(downloaded_repo_dir):\n",
    "        for root,dirs,files in os.walk(os.path.join(downloaded_repo_dir,repo_id)):\n",
    "            for file in files:\n",
    "                extension = get_file_extension(file)\n",
    "                try:\n",
    "                    if extensions_map[extension]:\n",
    "                        directory = os.path.join(root,file)\n",
    "                        df.loc[len(df)] = [directory, repo_id, file, extension]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    extensions_map[extension] = False\n",
    "        print(f\"Done with repo: {repo_id}\")\n",
    "    print(f\"Extensions: {extensions_map}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verilog_files_df = create_files_df('data/full_repos/permissive',verilog_extension_files + system_verilog_extension_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "verilog_files_df.to_csv('./files_index.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition dataset for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_index = pd.read_csv('data/search_repo_indices/files_index.csv',index_col=0)\n",
    "# files_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314877"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_index = files_index[files_index['extension'].isin(extensions_to_keep)]\n",
    "# files_index = files_index.reset_index(drop=True)\n",
    "len(files_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_index = files_index.reset_index(drop=True)\n",
    "# files_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_indices = np.random.choice(len(files_index),replace=False,size=200)\n",
    "remaining_files_index = files_index.drop(index=few_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314677"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_files_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_partitions = 10\n",
    "tot_len = len(remaining_files_index)\n",
    "for i in range(number_of_partitions):\n",
    "    partition_df = remaining_files_index.iloc[list(range(i*tot_len//number_of_partitions,(i+1)*tot_len//number_of_partitions))]\n",
    "    partition_df.to_csv(f\"data/verilog_partitions/files_index_part_{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_len = len(remaining_files_index)\n",
    "total = 0\n",
    "for i in range(number_of_partitions):\n",
    "    length = len(pd.read_csv(f\"data/verilog_partitions/files_index_part_{i}.csv\"))\n",
    "    total += length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill partitions with source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_source_code(directory):\n",
    "    # Biggest error = utf-8 encoding problem\n",
    "    try:\n",
    "        # return open(directory,'r').read()\n",
    "        with codecs.open(directory,encoding='utf-8', errors='replace', mode = 'r') as f:\n",
    "            data = f.read()\n",
    "        return data.replace(\"\\x00\",\"\") # replacing this might not be needed but someone online said it helps...\n",
    "    except Exception as e:\n",
    "        e_string = f\"0:FOUND ERROR: {e}\"\n",
    "        print(e_string)\n",
    "        return e_string\n",
    "\n",
    "def clean_row_directory(row):\n",
    "    return row['directory'].replace(\"\\\\\",\"/\")\n",
    "\n",
    "def add_source_code_to_index_df(df):\n",
    "    df['directory'] = df.apply(lambda row: clean_row_directory(row),axis=1)\n",
    "    df['code'] = \"\"\n",
    "    tqdm.pandas(desc='Apply read_source_code')\n",
    "    df['code'] = df.progress_apply(lambda row: read_source_code(row['directory']),axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Apply read_source_code:  94%|█████████▎| 29496/31468 [05:13<00:28, 70.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Apply read_source_code: 100%|██████████| 31468/31468 [05:48<00:00, 90.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "number_of_partitions = 10\n",
    "for i in range(number_of_partitions):\n",
    "    df_dir = f\"data/verilog_partitions/files_index_part_{i}.csv\"\n",
    "    print(f\"Starting {i}\")\n",
    "    partition_df = pd.read_csv(df_dir,index_col=0)\n",
    "    new_partition_df = add_source_code_to_index_df(partition_df)\n",
    "    new_partition_df.to_csv(df_dir)\n",
    "    del partition_df, new_partition_df\n",
    "print(\"All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('hdl_dataset_creation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d483179aadbe36b266083fb168142eacd02134ef8f8b2756794bec1efb632f92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
