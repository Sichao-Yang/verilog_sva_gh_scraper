{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "\n",
    "1. search for 'verilog' repos;\n",
    "2. keep repo with permissive license;\n",
    "3. download repo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 在python脚本里设置代理\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from github import Github\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "from os import path as osp\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import  os\n",
    "ACCESS_TOKEN = os.getenv('git_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'limit': 5000, 'used': 0, 'remaining': 5000, 'reset': 1714145251}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_rate_limit().core.raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stall requesting when reaching the limit (5000 requests per hour)\n",
    "def pre_github_request_checker():\n",
    "    rate_data = g.get_rate_limit().core.raw_data\n",
    "    if rate_data['remaining'] < 50:\n",
    "        time_to_reset = rate_data['reset'] - int(time.time()) + 1\n",
    "        print(f\"Sleeping for {time_to_reset} seconds\")\n",
    "        time.sleep(time_to_reset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_github(language, start_date, end_date, permissible_lang_list=['Verilog', 'SystemVerilog']):\n",
    "    \"\"\"\n",
    "    More info:\n",
    "    https://docs.github.com/en/search-github/getting-started-with-searching-on-github/understanding-the-search-syntax#query-for-dates\n",
    "    \"\"\"\n",
    "    assert language in permissible_lang_list\n",
    "    date_q = f\"{start_date.strftime('%Y-%m-%d')}..{end_date.strftime('%Y-%m-%d')}\" \n",
    "    result = g.search_repositories(\"\",language=language, created=date_q) \n",
    "    print(f\"Found {result.totalCount} repos for: {language}, {date_q}\")\n",
    "    return result\n",
    "\n",
    "# results = search_github('Verilog', datetime(1980,1,1), datetime(2010,1,1))\n",
    "# results = search_github('Verilog', datetime(1980,1,1), datetime(2015,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repo_to_df(df, repo, data_field):\n",
    "    data = [getattr(repo, attr) for attr in data_field]\n",
    "    data.append(repo.get_topics())\n",
    "    try:\n",
    "        license_url = repo.get_license().license.url\n",
    "    except:\n",
    "        license_url = \"None\"\n",
    "    data.append(license_url)\n",
    "    df.loc[len(df)] = data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get repo info, add to df\n",
    "def process_repo_search_results(df, results, data_field):\n",
    "    for i in range(1000):\n",
    "        rate_data = g.get_rate_limit().core.raw_data\n",
    "        now_seconds = int(time.time())\n",
    "        if rate_data['remaining'] < 100:\n",
    "            time_to_reset = rate_data['reset'] - int(time.time()) + 1\n",
    "            print(f\"Sleeping for {time_to_reset} seconds\")\n",
    "            time.sleep(time_to_reset)\n",
    "        page = results.get_page(i)\n",
    "        page_size = len(page)\n",
    "        for j in range(page_size):\n",
    "            df = add_repo_to_df(df, page[j], data_field)\n",
    "        if page_size < 30:\n",
    "            break   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path as osp\n",
    "\n",
    "def init_df(data_field):\n",
    "    return pd.DataFrame(columns=data_field + [\"topics\",\"license_url\"])\n",
    "\n",
    "def make_datetime(string, format='%Y-%m-%d'):\n",
    "    return datetime.strptime(string, format)\n",
    "\n",
    "def str_datetime(dt, format='%Y-%m-%d'):\n",
    "    return dt.strftime(format)\n",
    "\n",
    "def append_msg(msg, path):\n",
    "    with open(path, 'a') as fp:\n",
    "        fp.write(msg+'\\n')\n",
    "\n",
    "def find_repos(language, date_list, data_field, root_path='data/repo_index', search_limit=1000):\n",
    "    os.makedirs(root_path, exist_ok=True)\n",
    "    log_path = osp.join(root_path, 'log')\n",
    "    \n",
    "    while len(date_list)>0:\n",
    "        start_date, end_date = date_list.pop()\n",
    "        # search by lang, start & end dates\n",
    "        repo_search_results = search_github(language, start_date, end_date)\n",
    "        if repo_search_results.totalCount > 0:\n",
    "            if repo_search_results.totalCount >= search_limit:\n",
    "                # Reduce date range recursively\n",
    "                delta = (end_date - start_date) / 2\n",
    "                date_list.append((start_date + delta, end_date))\n",
    "                date_list.append((start_date, end_date - delta))\n",
    "                print(f\"Search count > {search_limit} ({repo_search_results.totalCount}) in ({start_date}, {end_date}), split to {date_list[-1]} and {date_list[-2]}\")\n",
    "            else:\n",
    "                df = init_df(data_field)\n",
    "                process_repo_search_results(df, repo_search_results, data_field)\n",
    "                file_name = f\"{language}_{str_datetime(start_date)}_{str_datetime(end_date)}.csv\"\n",
    "                df.to_csv(osp.join(root_path, file_name))\n",
    "                msg = f\"Done: {str_datetime(start_date)}..{str_datetime(end_date)}, length: {len(df)}\"\n",
    "                append_msg(msg, log_path)\n",
    "                print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_field = [\n",
    "    \"id\",\n",
    "    \"clone_url\",\n",
    "    \"created_at\",\n",
    "    \"description\",\n",
    "    \"full_name\",\n",
    "    \"language\",\n",
    "    \"name\",\n",
    "    \"size\",\n",
    "    \"stargazers_count\",\n",
    "    \"updated_at\",\n",
    "    \"forks_count\"\n",
    "]\n",
    "data_field.sort()\n",
    "language = \"Verilog\"\n",
    "\n",
    "# find_repos(language, [(datetime(2010,1,1), datetime(2010,5,1))], data_field, search_limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve all repo urls\n",
    "\n",
    "since github api's max rate limit per hour is 1000, so we need to split the search date range. you can either:\n",
    "\n",
    "1. set the whole range and let the function find_repo to do the binary split for you;\n",
    "2. or split manually (likely based on previous search experience) and pass the date_list to find_repo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. set full range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2000,1,1)\n",
    "end_date = datetime(2023,11,7)\n",
    "date_list = [(start_date, end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. set list of ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_num_date(text):\n",
    "    # Extracting number and dates using one pattern\n",
    "    pattern = r'Found (\\d+) repos for: \\w+, (\\d{4}-\\d{2}-\\d{2})\\.\\.(\\d{4}-\\d{2}-\\d{2})'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        number = match.group(1)\n",
    "        start_date = match.group(2)\n",
    "        end_date = match.group(3)\n",
    "        # print(\"Number:\", number)\n",
    "        # print(\"Start Date:\", start_date)\n",
    "        # print(\"End Date:\", end_date)\n",
    "        return (number, (start_date, end_date))\n",
    "    else:\n",
    "        # print(\"No match found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [clone_url, created_at, description, forks_count, full_name, id, language, name, size, stargazers_count, updated_at, topics, license_url]\n",
      "Index: []\n",
      "Found 1000 repos for: Verilog, 2000-01-01..2024-04-10\n",
      "Found 279 repos for: Verilog, 2000-01-01..2012-02-20\n",
      "Done: 2000-01-01..2012-02-20, df length: 279\n",
      "Found 1000 repos for: Verilog, 2012-02-20..2024-04-10\n",
      "Found 1000 repos for: Verilog, 2012-02-20..2018-03-17\n",
      "Found 1000 repos for: Verilog, 2012-02-20..2015-03-04\n",
      "Found 643 repos for: Verilog, 2012-02-20..2013-08-27\n"
     ]
    }
   ],
   "source": [
    "with open('github.log') as fp:\n",
    "    log = fp.readlines()\n",
    "for i in range(10):\n",
    "    print(log[i], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2014, 10, 17, 0, 0), datetime.datetime(2015, 3, 4, 0, 0)),\n",
      " (datetime.datetime(2014, 5, 31, 0, 0), datetime.datetime(2014, 10, 17, 0, 0)),\n",
      " (datetime.datetime(2013, 8, 27, 0, 0), datetime.datetime(2014, 5, 31, 0, 0)),\n",
      " (datetime.datetime(2012, 2, 20, 0, 0), datetime.datetime(2013, 8, 27, 0, 0)),\n",
      " (datetime.datetime(2000, 1, 1, 0, 0), datetime.datetime(2012, 2, 20, 0, 0))]\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "date_list = []\n",
    "for l in log:\n",
    "    ret = find_num_date(l)\n",
    "    if ret is not None:\n",
    "        if eval(ret[0]) != 1000:\n",
    "            date_list.append(\n",
    "                (make_datetime(ret[1][0]), make_datetime(ret[1][1]))\n",
    "                )\n",
    "            \n",
    "# reverse the list and pop from back\n",
    "date_list = date_list[::-1]\n",
    "pprint(date_list[-5:])\n",
    "print(len(date_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 repos for: Verilog, 2023-11-07..2024-04-13\n",
      "Search count > 1000 (1000) in (2023-11-07 00:00:00, 2024-04-13 19:46:51.460603), split to (datetime.datetime(2024, 1, 25, 9, 53, 25, 730302), datetime.datetime(2024, 4, 13, 19, 46, 51, 460603)) and (datetime.datetime(2023, 11, 7, 0, 0), datetime.datetime(2024, 1, 25, 9, 53, 25, 730301))\n",
      "Found 1000 repos for: Verilog, 2024-01-25..2024-04-13\n",
      "Search count > 1000 (1000) in (2024-01-25 09:53:25.730302, 2024-04-13 19:46:51.460603), split to (datetime.datetime(2024, 3, 5, 2, 50, 8, 595452), datetime.datetime(2024, 4, 13, 19, 46, 51, 460603)) and (datetime.datetime(2024, 1, 25, 9, 53, 25, 730302), datetime.datetime(2024, 3, 5, 2, 50, 8, 595453))\n",
      "Found 1000 repos for: Verilog, 2024-03-05..2024-04-13\n",
      "Search count > 1000 (1000) in (2024-03-05 02:50:08.595452, 2024-04-13 19:46:51.460603), split to (datetime.datetime(2024, 3, 24, 23, 18, 30, 28028), datetime.datetime(2024, 4, 13, 19, 46, 51, 460603)) and (datetime.datetime(2024, 3, 5, 2, 50, 8, 595452), datetime.datetime(2024, 3, 24, 23, 18, 30, 28027))\n",
      "Found 937 repos for: Verilog, 2024-03-24..2024-04-13\n",
      "Done: 2024-03-24..2024-04-13, length: 937\n",
      "Found 1000 repos for: Verilog, 2024-03-05..2024-03-24\n",
      "Search count > 1000 (1000) in (2024-03-05 02:50:08.595452, 2024-03-24 23:18:30.028027), split to (datetime.datetime(2024, 3, 15, 1, 4, 19, 311740), datetime.datetime(2024, 3, 24, 23, 18, 30, 28027)) and (datetime.datetime(2024, 3, 5, 2, 50, 8, 595452), datetime.datetime(2024, 3, 15, 1, 4, 19, 311739))\n",
      "Found 505 repos for: Verilog, 2024-03-15..2024-03-24\n",
      "Done: 2024-03-15..2024-03-24, length: 505\n",
      "Found 579 repos for: Verilog, 2024-03-05..2024-03-15\n",
      "Done: 2024-03-05..2024-03-15, length: 579\n",
      "Found 1000 repos for: Verilog, 2024-01-25..2024-03-05\n",
      "Search count > 1000 (1000) in (2024-01-25 09:53:25.730302, 2024-03-05 02:50:08.595453), split to (datetime.datetime(2024, 2, 14, 6, 21, 47, 162878), datetime.datetime(2024, 3, 5, 2, 50, 8, 595453)) and (datetime.datetime(2024, 1, 25, 9, 53, 25, 730302), datetime.datetime(2024, 2, 14, 6, 21, 47, 162877))\n",
      "Found 907 repos for: Verilog, 2024-02-14..2024-03-05\n",
      "Sleeping for 142 seconds\n",
      "Done: 2024-02-14..2024-03-05, length: 907\n",
      "Found 792 repos for: Verilog, 2024-01-25..2024-02-14\n",
      "Done: 2024-01-25..2024-02-14, length: 791\n",
      "Found 1000 repos for: Verilog, 2023-11-07..2024-01-25\n",
      "Search count > 1000 (1000) in (2023-11-07 00:00:00, 2024-01-25 09:53:25.730301), split to (datetime.datetime(2023, 12, 16, 16, 56, 42, 865150), datetime.datetime(2024, 1, 25, 9, 53, 25, 730301)) and (datetime.datetime(2023, 11, 7, 0, 0), datetime.datetime(2023, 12, 16, 16, 56, 42, 865151))\n",
      "Found 1000 repos for: Verilog, 2023-12-16..2024-01-25\n",
      "Search count > 1000 (1000) in (2023-12-16 16:56:42.865150, 2024-01-25 09:53:25.730301), split to (datetime.datetime(2024, 1, 5, 13, 25, 4, 297726), datetime.datetime(2024, 1, 25, 9, 53, 25, 730301)) and (datetime.datetime(2023, 12, 16, 16, 56, 42, 865150), datetime.datetime(2024, 1, 5, 13, 25, 4, 297725))\n",
      "Found 923 repos for: Verilog, 2024-01-05..2024-01-25\n",
      "Done: 2024-01-05..2024-01-25, length: 923\n",
      "Found 820 repos for: Verilog, 2023-12-16..2024-01-05\n",
      "Sleeping for 97 seconds\n",
      "Done: 2023-12-16..2024-01-05, length: 820\n",
      "Found 1000 repos for: Verilog, 2023-11-07..2023-12-16\n",
      "Search count > 1000 (1000) in (2023-11-07 00:00:00, 2023-12-16 16:56:42.865151), split to (datetime.datetime(2023, 11, 26, 20, 28, 21, 432576), datetime.datetime(2023, 12, 16, 16, 56, 42, 865151)) and (datetime.datetime(2023, 11, 7, 0, 0), datetime.datetime(2023, 11, 26, 20, 28, 21, 432575))\n",
      "Found 989 repos for: Verilog, 2023-11-26..2023-12-16\n",
      "Done: 2023-11-26..2023-12-16, length: 989\n",
      "Found 969 repos for: Verilog, 2023-11-07..2023-11-26\n",
      "Sleeping for 133 seconds\n",
      "Done: 2023-11-07..2023-11-26, length: 969\n"
     ]
    }
   ],
   "source": [
    "find_repos(language, date_list, data_field, root_path='data/repo_index', search_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 repos for: SystemVerilog, 2000-01-01..2023-11-07\n",
      "Search count > 1000 (1000) in (2000-01-01 00:00:00, 2023-11-07 00:00:00), split to (datetime.datetime(2011, 12, 4, 12, 0), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2000, 1, 1, 0, 0), datetime.datetime(2011, 12, 4, 12, 0))\n",
      "Found 1000 repos for: SystemVerilog, 2011-12-04..2023-11-07\n",
      "Search count > 1000 (1000) in (2011-12-04 12:00:00, 2023-11-07 00:00:00), split to (datetime.datetime(2017, 11, 20, 6, 0), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2011, 12, 4, 12, 0), datetime.datetime(2017, 11, 20, 6, 0))\n",
      "Found 1000 repos for: SystemVerilog, 2017-11-20..2023-11-07\n",
      "Search count > 1000 (1000) in (2017-11-20 06:00:00, 2023-11-07 00:00:00), split to (datetime.datetime(2020, 11, 13, 3, 0), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2017, 11, 20, 6, 0), datetime.datetime(2020, 11, 13, 3, 0))\n",
      "Found 1000 repos for: SystemVerilog, 2020-11-13..2023-11-07\n",
      "Search count > 1000 (1000) in (2020-11-13 03:00:00, 2023-11-07 00:00:00), split to (datetime.datetime(2022, 5, 11, 13, 30), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2020, 11, 13, 3, 0), datetime.datetime(2022, 5, 11, 13, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2022-05-11..2023-11-07\n",
      "Search count > 1000 (1000) in (2022-05-11 13:30:00, 2023-11-07 00:00:00), split to (datetime.datetime(2023, 2, 7, 18, 45), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2022, 5, 11, 13, 30), datetime.datetime(2023, 2, 7, 18, 45))\n",
      "Found 1000 repos for: SystemVerilog, 2023-02-07..2023-11-07\n",
      "Search count > 1000 (1000) in (2023-02-07 18:45:00, 2023-11-07 00:00:00), split to (datetime.datetime(2023, 6, 23, 21, 22, 30), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2023, 2, 7, 18, 45), datetime.datetime(2023, 6, 23, 21, 22, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2023-06-23..2023-11-07\n",
      "Search count > 1000 (1000) in (2023-06-23 21:22:30, 2023-11-07 00:00:00), split to (datetime.datetime(2023, 8, 30, 22, 41, 15), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2023, 6, 23, 21, 22, 30), datetime.datetime(2023, 8, 30, 22, 41, 15))\n",
      "Found 1000 repos for: SystemVerilog, 2023-08-30..2023-11-07\n",
      "Search count > 1000 (1000) in (2023-08-30 22:41:15, 2023-11-07 00:00:00), split to (datetime.datetime(2023, 10, 3, 23, 20, 37, 500000), datetime.datetime(2023, 11, 7, 0, 0)) and (datetime.datetime(2023, 8, 30, 22, 41, 15), datetime.datetime(2023, 10, 3, 23, 20, 37, 500000))\n",
      "Found 502 repos for: SystemVerilog, 2023-10-03..2023-11-07\n",
      "Sleeping for 68 seconds\n",
      "Done: 2023-10-03..2023-11-07, length: 502\n",
      "Found 514 repos for: SystemVerilog, 2023-08-30..2023-10-03\n",
      "Done: 2023-08-30..2023-10-03, length: 514\n",
      "Found 752 repos for: SystemVerilog, 2023-06-23..2023-08-30\n",
      "Done: 2023-06-23..2023-08-30, length: 752\n",
      "Found 1000 repos for: SystemVerilog, 2023-02-07..2023-06-23\n",
      "Search count > 1000 (1000) in (2023-02-07 18:45:00, 2023-06-23 21:22:30), split to (datetime.datetime(2023, 4, 16, 20, 3, 45), datetime.datetime(2023, 6, 23, 21, 22, 30)) and (datetime.datetime(2023, 2, 7, 18, 45), datetime.datetime(2023, 4, 16, 20, 3, 45))\n",
      "Found 743 repos for: SystemVerilog, 2023-04-16..2023-06-23\n",
      "Done: 2023-04-16..2023-06-23, length: 743\n",
      "Found 816 repos for: SystemVerilog, 2023-02-07..2023-04-16\n",
      "Sleeping for 155 seconds\n",
      "Done: 2023-02-07..2023-04-16, length: 816\n",
      "Found 1000 repos for: SystemVerilog, 2022-05-11..2023-02-07\n",
      "Search count > 1000 (1000) in (2022-05-11 13:30:00, 2023-02-07 18:45:00), split to (datetime.datetime(2022, 9, 24, 16, 7, 30), datetime.datetime(2023, 2, 7, 18, 45)) and (datetime.datetime(2022, 5, 11, 13, 30), datetime.datetime(2022, 9, 24, 16, 7, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2022-09-24..2023-02-07\n",
      "Search count > 1000 (1000) in (2022-09-24 16:07:30, 2023-02-07 18:45:00), split to (datetime.datetime(2022, 12, 1, 17, 26, 15), datetime.datetime(2023, 2, 7, 18, 45)) and (datetime.datetime(2022, 9, 24, 16, 7, 30), datetime.datetime(2022, 12, 1, 17, 26, 15))\n",
      "Found 572 repos for: SystemVerilog, 2022-12-01..2023-02-07\n",
      "Done: 2022-12-01..2023-02-07, length: 572\n",
      "Found 621 repos for: SystemVerilog, 2022-09-24..2022-12-01\n",
      "Done: 2022-09-24..2022-12-01, length: 621\n",
      "Found 998 repos for: SystemVerilog, 2022-05-11..2022-09-24\n",
      "Sleeping for 152 seconds\n",
      "Done: 2022-05-11..2022-09-24, length: 998\n",
      "Found 1000 repos for: SystemVerilog, 2020-11-13..2022-05-11\n",
      "Search count > 1000 (1000) in (2020-11-13 03:00:00, 2022-05-11 13:30:00), split to (datetime.datetime(2021, 8, 12, 8, 15), datetime.datetime(2022, 5, 11, 13, 30)) and (datetime.datetime(2020, 11, 13, 3, 0), datetime.datetime(2021, 8, 12, 8, 15))\n",
      "Found 1000 repos for: SystemVerilog, 2021-08-12..2022-05-11\n",
      "Search count > 1000 (1000) in (2021-08-12 08:15:00, 2022-05-11 13:30:00), split to (datetime.datetime(2021, 12, 26, 10, 52, 30), datetime.datetime(2022, 5, 11, 13, 30)) and (datetime.datetime(2021, 8, 12, 8, 15), datetime.datetime(2021, 12, 26, 10, 52, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2021-12-26..2022-05-11\n",
      "Search count > 1000 (1000) in (2021-12-26 10:52:30, 2022-05-11 13:30:00), split to (datetime.datetime(2022, 3, 4, 12, 11, 15), datetime.datetime(2022, 5, 11, 13, 30)) and (datetime.datetime(2021, 12, 26, 10, 52, 30), datetime.datetime(2022, 3, 4, 12, 11, 15))\n",
      "Found 620 repos for: SystemVerilog, 2022-03-04..2022-05-11\n",
      "Done: 2022-03-04..2022-05-11, length: 620\n",
      "Found 434 repos for: SystemVerilog, 2021-12-26..2022-03-04\n",
      "Done: 2021-12-26..2022-03-04, length: 434\n",
      "Found 873 repos for: SystemVerilog, 2021-08-12..2021-12-26\n",
      "Done: 2021-08-12..2021-12-26, length: 873\n",
      "Found 1000 repos for: SystemVerilog, 2020-11-13..2021-08-12\n",
      "Search count > 1000 (1000) in (2020-11-13 03:00:00, 2021-08-12 08:15:00), split to (datetime.datetime(2021, 3, 29, 5, 37, 30), datetime.datetime(2021, 8, 12, 8, 15)) and (datetime.datetime(2020, 11, 13, 3, 0), datetime.datetime(2021, 3, 29, 5, 37, 30))\n",
      "Found 822 repos for: SystemVerilog, 2021-03-29..2021-08-12\n",
      "Sleeping for 109 seconds\n",
      "Done: 2021-03-29..2021-08-12, length: 822\n",
      "Found 758 repos for: SystemVerilog, 2020-11-13..2021-03-29\n",
      "Done: 2020-11-13..2021-03-29, length: 758\n",
      "Found 1000 repos for: SystemVerilog, 2017-11-20..2020-11-13\n",
      "Search count > 1000 (1000) in (2017-11-20 06:00:00, 2020-11-13 03:00:00), split to (datetime.datetime(2019, 5, 18, 16, 30), datetime.datetime(2020, 11, 13, 3, 0)) and (datetime.datetime(2017, 11, 20, 6, 0), datetime.datetime(2019, 5, 18, 16, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2019-05-18..2020-11-13\n",
      "Search count > 1000 (1000) in (2019-05-18 16:30:00, 2020-11-13 03:00:00), split to (datetime.datetime(2020, 2, 14, 21, 45), datetime.datetime(2020, 11, 13, 3, 0)) and (datetime.datetime(2019, 5, 18, 16, 30), datetime.datetime(2020, 2, 14, 21, 45))\n",
      "Found 1000 repos for: SystemVerilog, 2020-02-14..2020-11-13\n",
      "Search count > 1000 (1000) in (2020-02-14 21:45:00, 2020-11-13 03:00:00), split to (datetime.datetime(2020, 6, 30, 0, 22, 30), datetime.datetime(2020, 11, 13, 3, 0)) and (datetime.datetime(2020, 2, 14, 21, 45), datetime.datetime(2020, 6, 30, 0, 22, 30))\n",
      "Found 644 repos for: SystemVerilog, 2020-06-30..2020-11-13\n",
      "Done: 2020-06-30..2020-11-13, length: 644\n",
      "Found 662 repos for: SystemVerilog, 2020-02-14..2020-06-30\n",
      "Sleeping for 129 seconds\n",
      "Done: 2020-02-14..2020-06-30, length: 662\n",
      "Found 1000 repos for: SystemVerilog, 2019-05-18..2020-02-14\n",
      "Search count > 1000 (1000) in (2019-05-18 16:30:00, 2020-02-14 21:45:00), split to (datetime.datetime(2019, 10, 1, 19, 7, 30), datetime.datetime(2020, 2, 14, 21, 45)) and (datetime.datetime(2019, 5, 18, 16, 30), datetime.datetime(2019, 10, 1, 19, 7, 30))\n",
      "Found 563 repos for: SystemVerilog, 2019-10-01..2020-02-14\n",
      "Done: 2019-10-01..2020-02-14, length: 563\n",
      "Found 501 repos for: SystemVerilog, 2019-05-18..2019-10-01\n",
      "Done: 2019-05-18..2019-10-01, length: 501\n",
      "Found 1000 repos for: SystemVerilog, 2017-11-20..2019-05-18\n",
      "Search count > 1000 (1000) in (2017-11-20 06:00:00, 2019-05-18 16:30:00), split to (datetime.datetime(2018, 8, 19, 11, 15), datetime.datetime(2019, 5, 18, 16, 30)) and (datetime.datetime(2017, 11, 20, 6, 0), datetime.datetime(2018, 8, 19, 11, 15))\n",
      "Found 932 repos for: SystemVerilog, 2018-08-19..2019-05-18\n",
      "Done: 2018-08-19..2019-05-18, length: 932\n",
      "Found 689 repos for: SystemVerilog, 2017-11-20..2018-08-19\n",
      "Sleeping for 98 seconds\n",
      "Done: 2017-11-20..2018-08-19, length: 689\n",
      "Found 1000 repos for: SystemVerilog, 2011-12-04..2017-11-20\n",
      "Search count > 1000 (1000) in (2011-12-04 12:00:00, 2017-11-20 06:00:00), split to (datetime.datetime(2014, 11, 27, 9, 0), datetime.datetime(2017, 11, 20, 6, 0)) and (datetime.datetime(2011, 12, 4, 12, 0), datetime.datetime(2014, 11, 27, 9, 0))\n",
      "Found 1000 repos for: SystemVerilog, 2014-11-27..2017-11-20\n",
      "Search count > 1000 (1000) in (2014-11-27 09:00:00, 2017-11-20 06:00:00), split to (datetime.datetime(2016, 5, 24, 19, 30), datetime.datetime(2017, 11, 20, 6, 0)) and (datetime.datetime(2014, 11, 27, 9, 0), datetime.datetime(2016, 5, 24, 19, 30))\n",
      "Found 1000 repos for: SystemVerilog, 2016-05-24..2017-11-20\n",
      "Search count > 1000 (1000) in (2016-05-24 19:30:00, 2017-11-20 06:00:00), split to (datetime.datetime(2017, 2, 21, 0, 45), datetime.datetime(2017, 11, 20, 6, 0)) and (datetime.datetime(2016, 5, 24, 19, 30), datetime.datetime(2017, 2, 21, 0, 45))\n",
      "Found 557 repos for: SystemVerilog, 2017-02-21..2017-11-20\n",
      "Done: 2017-02-21..2017-11-20, length: 557\n",
      "Found 446 repos for: SystemVerilog, 2016-05-24..2017-02-21\n",
      "Done: 2016-05-24..2017-02-21, length: 446\n",
      "Found 487 repos for: SystemVerilog, 2014-11-27..2016-05-24\n",
      "Done: 2014-11-27..2016-05-24, length: 487\n",
      "Found 141 repos for: SystemVerilog, 2011-12-04..2014-11-27\n",
      "Done: 2011-12-04..2014-11-27, length: 141\n",
      "Found 4 repos for: SystemVerilog, 2000-01-01..2011-12-04\n",
      "Done: 2000-01-01..2011-12-04, length: 4\n"
     ]
    }
   ],
   "source": [
    "language = 'SystemVerilog'\n",
    "find_repos(language, date_list, data_field, root_path='data/repo_index', search_limit=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concate repo dfs and deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_deduplicate_gh_search_results(csvs):\n",
    "    df = pd.concat(map(lambda x: pd.read_csv(x, na_values=['None']), csvs), ignore_index=True)\n",
    "    df = df.drop(['Unnamed: 0'],axis=1)\n",
    "    df = df.drop_duplicates([c for c in df.columns if c != 'updated_at'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "28\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "from os import path as osp\n",
    "import os\n",
    "\n",
    "raw_repo_indices_dir = 'data/repo_index_raw'\n",
    "full_idx = os.listdir(raw_repo_indices_dir)\n",
    "\n",
    "v_filepaths = [osp.join(raw_repo_indices_dir, f) for f in full_idx if 'SystemVerilog' not in f and '.csv' in f]\n",
    "sv_filepaths = [osp.join(raw_repo_indices_dir, f) for f in full_idx if 'SystemVerilog' in f and '.csv' in f]\n",
    "print(len(full_idx))\n",
    "print(len(sv_filepaths))\n",
    "print(len(v_filepaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71603\n",
      "16989\n",
      "88592\n"
     ]
    }
   ],
   "source": [
    "verilog_df = combine_and_deduplicate_gh_search_results(v_filepaths)\n",
    "systemverilog_df = combine_and_deduplicate_gh_search_results(sv_filepaths)\n",
    "\n",
    "print(len(verilog_df))\n",
    "# verilog_df.head()\n",
    "print(len(systemverilog_df))\n",
    "print(len(verilog_df)+len(systemverilog_df))\n",
    "\n",
    "repo_indices_dir = 'data/repo_indices'\n",
    "os.makedirs(repo_indices_dir)\n",
    "verilog_df.to_csv(osp.join(repo_indices_dir,\"full_verilog_repos.csv\"))\n",
    "systemverilog_df.to_csv(osp.join(repo_indices_dir,\"full_systemverilog_repos.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so total repo count is: 71731+16989=88720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clone_url</th>\n",
       "      <th>created_at</th>\n",
       "      <th>description</th>\n",
       "      <th>forks_count</th>\n",
       "      <th>full_name</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>stargazers_count</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>topics</th>\n",
       "      <th>license_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74208</th>\n",
       "      <td>https://github.com/RavinduRepo/VerilogLabs.git</td>\n",
       "      <td>2024-04-09 13:28:06+00:00</td>\n",
       "      <td>Verilog Labs from university computer architec...</td>\n",
       "      <td>1</td>\n",
       "      <td>RavinduRepo/VerilogLabs</td>\n",
       "      <td>784246353</td>\n",
       "      <td>Verilog</td>\n",
       "      <td>VerilogLabs</td>\n",
       "      <td>2692</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-09 13:51:24+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74209</th>\n",
       "      <td>https://github.com/chaozhang2000/HI-CGRA-Sim.git</td>\n",
       "      <td>2024-04-12 10:19:20+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>chaozhang2000/HI-CGRA-Sim</td>\n",
       "      <td>785647818</td>\n",
       "      <td>Verilog</td>\n",
       "      <td>HI-CGRA-Sim</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-12 10:22:02+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74210</th>\n",
       "      <td>https://github.com/alexpenne1/350FP.git</td>\n",
       "      <td>2024-04-03 19:21:14+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>alexpenne1/350FP</td>\n",
       "      <td>781646936</td>\n",
       "      <td>Verilog</td>\n",
       "      <td>350FP</td>\n",
       "      <td>111989</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-03 19:27:06+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74211</th>\n",
       "      <td>https://github.com/GoodKook/ETRI-0.5u-CMOS-MPW...</td>\n",
       "      <td>2024-03-26 03:43:33+00:00</td>\n",
       "      <td>FIR Filter</td>\n",
       "      <td>0</td>\n",
       "      <td>GoodKook/ETRI-0.5u-CMOS-MPW-DK-Example--FIR8</td>\n",
       "      <td>777546797</td>\n",
       "      <td>Verilog</td>\n",
       "      <td>ETRI-0.5u-CMOS-MPW-DK-Example--FIR8</td>\n",
       "      <td>11258</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-26 07:36:20+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://api.github.com/licenses/gpl-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74212</th>\n",
       "      <td>https://github.com/kido2k3/FLOATING-POINTING_A...</td>\n",
       "      <td>2024-04-12 00:32:46+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>kido2k3/FLOATING-POINTING_ALU</td>\n",
       "      <td>785474221</td>\n",
       "      <td>Verilog</td>\n",
       "      <td>FLOATING-POINTING_ALU</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-12 00:49:32+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clone_url  \\\n",
       "74208     https://github.com/RavinduRepo/VerilogLabs.git   \n",
       "74209   https://github.com/chaozhang2000/HI-CGRA-Sim.git   \n",
       "74210            https://github.com/alexpenne1/350FP.git   \n",
       "74211  https://github.com/GoodKook/ETRI-0.5u-CMOS-MPW...   \n",
       "74212  https://github.com/kido2k3/FLOATING-POINTING_A...   \n",
       "\n",
       "                      created_at  \\\n",
       "74208  2024-04-09 13:28:06+00:00   \n",
       "74209  2024-04-12 10:19:20+00:00   \n",
       "74210  2024-04-03 19:21:14+00:00   \n",
       "74211  2024-03-26 03:43:33+00:00   \n",
       "74212  2024-04-12 00:32:46+00:00   \n",
       "\n",
       "                                             description  forks_count  \\\n",
       "74208  Verilog Labs from university computer architec...            1   \n",
       "74209                                                NaN            0   \n",
       "74210                                                NaN            0   \n",
       "74211                                         FIR Filter            0   \n",
       "74212                                                NaN            0   \n",
       "\n",
       "                                          full_name         id language  \\\n",
       "74208                       RavinduRepo/VerilogLabs  784246353  Verilog   \n",
       "74209                     chaozhang2000/HI-CGRA-Sim  785647818  Verilog   \n",
       "74210                              alexpenne1/350FP  781646936  Verilog   \n",
       "74211  GoodKook/ETRI-0.5u-CMOS-MPW-DK-Example--FIR8  777546797  Verilog   \n",
       "74212                 kido2k3/FLOATING-POINTING_ALU  785474221  Verilog   \n",
       "\n",
       "                                      name    size  stargazers_count  \\\n",
       "74208                          VerilogLabs    2692                 0   \n",
       "74209                          HI-CGRA-Sim      47                 0   \n",
       "74210                                350FP  111989                 0   \n",
       "74211  ETRI-0.5u-CMOS-MPW-DK-Example--FIR8   11258                 0   \n",
       "74212                FLOATING-POINTING_ALU     144                 0   \n",
       "\n",
       "                      updated_at topics  \\\n",
       "74208  2024-04-09 13:51:24+00:00     []   \n",
       "74209  2024-04-12 10:22:02+00:00     []   \n",
       "74210  2024-04-03 19:27:06+00:00     []   \n",
       "74211  2024-03-26 07:36:20+00:00     []   \n",
       "74212  2024-04-12 00:49:32+00:00     []   \n",
       "\n",
       "                                   license_url  \n",
       "74208                                      NaN  \n",
       "74209                                      NaN  \n",
       "74210                                      NaN  \n",
       "74211  https://api.github.com/licenses/gpl-3.0  \n",
       "74212                                      NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verilog_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88592\n"
     ]
    }
   ],
   "source": [
    "all_df = pd.concat([verilog_df,systemverilog_df]).drop_duplicates(subset=[c for c in verilog_df.columns if not c in ['language']])\n",
    "all_df.to_csv(\"data/all_deduplicated_repos.csv\")\n",
    "print(len(all_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding licenses and remove non permissive repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_licenses_from_repo_df_to_dict(repo_df, licenses_dict):\n",
    "    df_with_unique_licenses = repo_df.loc[repo_df['license_url'].dropna().drop_duplicates().index]\n",
    "    repo_ids = list(df_with_unique_licenses['id'])\n",
    "    for rid in repo_ids:\n",
    "        pre_github_request_checker()\n",
    "        license_data = g.get_repo(rid).get_license().license.raw_data\n",
    "        licenses_dict[license_data['url']] = license_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "licenses_dict = {}\n",
    "add_licenses_from_repo_df_to_dict(verilog_df,licenses_dict)\n",
    "add_licenses_from_repo_df_to_dict(systemverilog_df,licenses_dict)\n",
    "df = pd.DataFrame.from_dict(licenses_dict, orient='index')\n",
    "df.to_csv(os.path.join('data/search_repo_indices','licenses.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_licenses_with_permissions_conditions(license_df,permissions=[],conditions=[]):\n",
    "    indices = []\n",
    "    for i, row in license_df.iterrows():\n",
    "        if len(permissions) == 0 or set(permissions).issubset(set(row['permissions'])):\n",
    "            if len(conditions) == 0 or len(set(conditions).intersection(set(row['conditions']))) > 0 :\n",
    "                indices.append(i)\n",
    "    return license_df.loc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "permissions = ['modifications','distribution']\n",
    "special_conditions = ['same-license--file','same-license--library','same-license']\n",
    "permissive_licenses_df = get_licenses_with_permissions_conditions(df,permissions=permissions,conditions=[])\n",
    "distributive_licenses_df = get_licenses_with_permissions_conditions(df,permissions=['distribution'],conditions=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(permissive_licenses_df))\n",
    "print(len(distributive_licenses_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion, all licenses found are permissive in that they allow modifications and distribution! Repos without licenses are not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sv_df = systemverilog_df.dropna(subset=['license_url'])\n",
    "p_ve_df = verilog_df.dropna(subset=['license_url'])\n",
    "\n",
    "p_sv_df.to_csv(os.path.join(repo_indices_dir,\"permissive_systemverilog_repos.csv\"))\n",
    "p_ve_df.to_csv(os.path.join(repo_indices_dir,\"permissive_verilog_repos.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_all_df = pd.concat([p_ve_df,p_sv_df]).drop_duplicates(subset=[c for c in verilog_df.columns if not c in ['language']])\n",
    "p_all_df.to_csv(\"data/permissive_all_deduplicated_repos.csv\")\n",
    "print(len(p_all_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)\n",
    "all_permissions = []\n",
    "all_conditions = []\n",
    "all_limitations = []\n",
    "for i,row in df.iterrows():\n",
    "    all_permissions.append(row['permissions'])\n",
    "    all_conditions.append(row['conditions'])\n",
    "    all_limitations.append(row['limitations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['include-copyright', 'document-changes', 'disclose-source', 'same-license'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['include-copyright', 'document-changes', 'disclose-source', 'same-license'],\n",
       " ['include-copyright'],\n",
       " ['include-copyright'],\n",
       " ['include-copyright',\n",
       "  'disclose-source',\n",
       "  'document-changes',\n",
       "  'same-license--library'],\n",
       " ['include-copyright',\n",
       "  'disclose-source',\n",
       "  'document-changes',\n",
       "  'same-license--library'],\n",
       " [],\n",
       " ['include-copyright'],\n",
       " ['include-copyright',\n",
       "  'document-changes',\n",
       "  'disclose-source',\n",
       "  'network-use-disclose',\n",
       "  'same-license'],\n",
       " [],\n",
       " ['include-copyright'],\n",
       " [],\n",
       " ['disclose-source', 'include-copyright', 'same-license--file'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['include-copyright', 'document-changes', 'same-license'],\n",
       " ['disclose-source', 'include-copyright', 'same-license'],\n",
       " ['include-copyright--source'],\n",
       " ['include-copyright--source', 'document-changes'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['disclose-source', 'include-copyright', 'same-license'],\n",
       " ['include-copyright']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conditions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download repos:\n",
    "\n",
    "There are two ways:\n",
    "1. Clone or download full repo, then filter out files;\n",
    "2. get into repos and download single files if its extension is the target ones.\n",
    "\n",
    "The first method requires more storage space and network usage, but the second method requires more github api requests (for each file, it requires one get_repo search + n get_contents search).\n",
    "\n",
    "Therefore, the first method is prefered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before get_contents requests: (5000, 5000)\n",
      "[ContentFile(path=\".gitignore\"), ContentFile(path=\".gitmodules\"), ContentFile(path=\"LICENSE.FDL\"), ContentFile(path=\"LICENSE.GD\"), ContentFile(path=\"LICENSE.GPL\"), ContentFile(path=\"LICENSE.LGPL\"), ContentFile(path=\"Makefile\"), ContentFile(path=\"README\"), ContentFile(path=\"boards\"), ContentFile(path=\"cores\"), ContentFile(path=\"doc\"), ContentFile(path=\"softusb-input\"), ContentFile(path=\"software\"), ContentFile(path=\"tools\")]\n",
      "{'name': '.gitignore', 'path': '.gitignore', 'sha': '420345b3f2c148ebec1d7007cf1cc5f9bf0bc43e', 'size': 401, 'url': 'https://api.github.com/repos/m-labs/milkymist/contents/.gitignore?ref=master', 'html_url': 'https://github.com/m-labs/milkymist/blob/master/.gitignore', 'git_url': 'https://api.github.com/repos/m-labs/milkymist/git/blobs/420345b3f2c148ebec1d7007cf1cc5f9bf0bc43e', 'download_url': 'https://raw.githubusercontent.com/m-labs/milkymist/master/.gitignore', 'type': 'file', 'content': 'Ki5bb2FdCiouYml0CioubG9nCiouc3dwCiouZWxmCiouYmluCiouZnBnCiou\\ncmF3CiouZmJpCmJvYXJkcy9taWxreW1pc3Qtb25lL3N5bnRoZXNpcy9idWls\\nZApib2FyZHMvbWlsa3ltaXN0LW9uZS9zeW50aGVzaXMvYnVpbGQtcmVzY3Vl\\nCmJvYXJkcy9taWxreW1pc3Qtb25lL3N0YW5kYnkvYnVpbGQKc29mdHdhcmUv\\nZGVtby9sb2dvLmgKc29mdHdhcmUvbGliZnB2bS9wYXJzZXIuYwpzb2Z0d2Fy\\nZS9saWJmcHZtL3BhcnNlci5oCnNvZnR3YXJlL2xpYmZwdm0vcGFyc2VyLm91\\ndApzb2Z0d2FyZS9saWJmcHZtL3NjYW5uZXIuYwpzb2Z0d2FyZS9saWJoYWwv\\nc29mdHVzYi1pbnB1dC5oCnRvb2xzL2JpbjJoZXgKdG9vbHMvYnl0ZXN3YXAK\\ndG9vbHMvZmx0ZXJtCnRvb2xzL21ha2VyYXcKdG9vbHMvbWttbWltZwo=\\n', 'encoding': 'base64', '_links': {'self': 'https://api.github.com/repos/m-labs/milkymist/contents/.gitignore?ref=master', 'git': 'https://api.github.com/repos/m-labs/milkymist/git/blobs/420345b3f2c148ebec1d7007cf1cc5f9bf0bc43e', 'html': 'https://github.com/m-labs/milkymist/blob/master/.gitignore'}}\n",
      "after get_contents requests: (4997, 5000)\n"
     ]
    }
   ],
   "source": [
    "# here we used 3 requests in one repo get and one get_contents for root path\n",
    "# if we need to traversal the project, more get_contents are needed\n",
    "print(f\"before get_contents requests: {g.rate_limiting}\")\n",
    "repo = g.get_repo(279998)\n",
    "contents = repo.get_contents(\"/\")\n",
    "print(contents)\n",
    "print(contents[0].raw_data)\n",
    "print(f\"after get_contents requests: {g.rate_limiting}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. raw git clone\n",
    "\n",
    "1.1 create downloading bash script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'git clone --depth 1 --no-tags https://github.com/mrehkopf/sd2snes.git ./data/full_repos/exp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_clone_command(clone_url, directory, depth=1, branch=''):\n",
    "    command_parts = [f'git clone']\n",
    "    # TODO: rerun search without master branch (causes error for some searches!)\n",
    "    # if branch is not set, then the clone method will download default branch\n",
    "    if branch!='':\n",
    "        command_parts.append(f\"-b {branch}\") \n",
    "    command_parts.append(f\"--depth {depth}\")\n",
    "    command_parts.append(f\"--no-tags\")\n",
    "    # command_parts.append(f\"--no-checkout\")\n",
    "    command_parts.append(clone_url)\n",
    "    command_parts.append(directory)\n",
    "    return \" \".join(command_parts)\n",
    "\n",
    "def create_clone_script_for_df(repo_df, clone_out_dir, script_out_dir, every_k=100):\n",
    "    def get_script_out_path(script_count):\n",
    "        return osp.join(script_out_dir, 'clone_repo_{}.sh'.format(script_count))\n",
    "    script_count = 0\n",
    "    script_out_path = get_script_out_path(script_count)\n",
    "    for i,row in repo_df.iterrows():\n",
    "        clone_url = row['clone_url']\n",
    "        out_dir = os.path.join(clone_out_dir,str(row['id']))\n",
    "        # if not osp.exists(out_dir):\n",
    "        #     os.mkdir(out_dir)\n",
    "        if (i+1) % every_k == 0:\n",
    "            script_count+=1\n",
    "            script_out_path = get_script_out_path(script_count)\n",
    "        with open(script_out_path,'a') as f:\n",
    "            f.write(create_clone_command(clone_url,str(os.path.abspath(out_dir))).replace(\"\\\\\",\"/\") + \"\\n\")\n",
    "\n",
    "create_clone_command(\"https://github.com/mrehkopf/sd2snes.git\",\"./data/full_repos/exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88592"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.read_csv('data/all_deduplicated_repos.csv')\n",
    "len(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# the permissive filtering skipped here:\n",
    "full_repo_dir = \"data/full_repos\"\n",
    "scripts_dir = \"data/clone_scripts\"\n",
    "os.makedirs(full_repo_dir, exist_ok=True)\n",
    "if osp.exists(scripts_dir):\n",
    "    shutil.rmtree(scripts_dir, ignore_errors=False, onerror=None)\n",
    "os.makedirs(scripts_dir)\n",
    "create_clone_script_for_df(all_df, full_repo_dir, scripts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1692174</td>\n",
       "      <td>Arlet/verilog-6502</td>\n",
       "      <td>A Verilog HDL model of the MOS 6502 CPU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>583558</td>\n",
       "      <td>marmolejo/zet</td>\n",
       "      <td>Open source implementation of a x86 processor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279998</td>\n",
       "      <td>m-labs/milkymist</td>\n",
       "      <td>SoC design for Milkymist One - LM32, DDR SDRAM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143278</td>\n",
       "      <td>steveicarus/ivtest</td>\n",
       "      <td>Regression test suite for Icarus Verilog. (OBS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1911999</td>\n",
       "      <td>teknohog/Xilinx-Serial-Miner</td>\n",
       "      <td>Bitcoin miner for Xilinx FPGAs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                     full_name  \\\n",
       "0  1692174            Arlet/verilog-6502   \n",
       "1   583558                 marmolejo/zet   \n",
       "2   279998              m-labs/milkymist   \n",
       "3   143278            steveicarus/ivtest   \n",
       "4  1911999  teknohog/Xilinx-Serial-Miner   \n",
       "\n",
       "                                         description  \n",
       "0            A Verilog HDL model of the MOS 6502 CPU  \n",
       "1      Open source implementation of a x86 processor  \n",
       "2  SoC design for Milkymist One - LM32, DDR SDRAM...  \n",
       "3  Regression test suite for Icarus Verilog. (OBS...  \n",
       "4                     Bitcoin miner for Xilinx FPGAs  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_map = all_df[['id','full_name','description']]\n",
    "name_map.to_csv('data/name_map.csv')\n",
    "name_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define extensions to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.intel.com/content/www/us/en/programmable/quartushelp/17.0/reference/glossary/glosslist.htm\n",
    "# https://marketplace.visualstudio.com/items?itemName=eirikpre.systemverilog\n",
    "\n",
    "# log, tcl, makefile, readme are also not included:\n",
    "verilog_extension_files = ['v','verilog','vlg','vh']\n",
    "system_verilog_extension_files = ['sv','svh','svp', 'sva']\n",
    "extra_file_types = ['vo','vt'] # verilog output, verilog test bench\n",
    "\n",
    "extensions_to_keep = verilog_extension_files + system_verilog_extension_files + extra_file_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter repos\n",
    "\n",
    "you can do it after all download down, you can also do it along while downloading repos\n",
    "\n",
    "for total ~80000+ repos, the size of 10000 repos is around 300GB. So if dont filter, the full size can be around 2.4TB, which is too big to handle for this machine.\n",
    "\n",
    "update: after filtering, 300GB -> <100GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument '1' in rsplit can limit the split to max 2 segments\n",
    "def get_file_extension(path):\n",
    "    return path.rsplit(\".\",1)[-1]\n",
    "\n",
    "def delete_all_files_without_right_extension(start_dir, extensions_to_keep):\n",
    "    errors = []\n",
    "    for root, dirs, files in os.walk(start_dir):\n",
    "        for file in [sf for sf in files if not get_file_extension(sf) in extensions_to_keep]:\n",
    "            file_path = os.path.join(root,file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "            except Exception as e:\n",
    "                errors.append(str(e))\n",
    "    return errors\n",
    "\n",
    "def delete_empty_dirs(start_dir):\n",
    "    errors = []\n",
    "    for root, dirs, files in os.walk(start_dir,topdown=False):\n",
    "        for d in dirs:\n",
    "            dir_path = os.path.join(root,d)\n",
    "            if len(os.listdir(dir_path)) == 0:\n",
    "                try:\n",
    "                    os.rmdir(dir_path)\n",
    "                except Exception as e:\n",
    "                    errors.append(str(e))\n",
    "    return errors\n",
    "\n",
    "def filter_files_per_repo(repo_dir, extensions_to_keep):\n",
    "    error_list = delete_all_files_without_right_extension(repo_dir, extensions_to_keep)\n",
    "    repo_id = repo_dir.rsplit('/',1)[-1]\n",
    "    with open(f'logs/repo_deletion_error_{repo_id}.log', 'a') as f:\n",
    "        for e in error_list:\n",
    "            f.write(e+'\\n')\n",
    "    errs = delete_empty_dirs(repo_dir)\n",
    "    if len(errs)>0:\n",
    "        error_list.extend(errs)\n",
    "    if len(os.listdir(repo_dir)) == 0:\n",
    "        try:\n",
    "            os.rmdir(dir_path)\n",
    "        except Exception as e:\n",
    "            error_list.append(str(e))\n",
    "    with open(f'logs/repo_deletion_error_{repo_id}.log', 'w+') as f:\n",
    "        for e in error_list:\n",
    "            f.write(e+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10328/10328 [38:15<00:00,  4.50it/s] \n"
     ]
    }
   ],
   "source": [
    "# do it after download completed:\n",
    "# start_dir = \"data/full_repos\"\n",
    "# files_errors = delete_all_files_without_right_extension(start_dir, extensions_to_keep)\n",
    "# dirs_errors = delete_empty_dirs(start_dir)\n",
    "\n",
    "# do it while downloading:\n",
    "root_repos = 'data/full_repos'\n",
    "for repo in tqdm(os.listdir(root_repos)):\n",
    "    repo_dir =  root_repos+'/'+repo\n",
    "    filter_files_per_repo(repo_dir, extensions_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import subprocess\n",
    "\n",
    "def kill_orphan_p(patten=\"\"):\n",
    "    # kill orphan process with specified grep pattern\n",
    "    base = \"ps aux|grep {}\".format(patten)\n",
    "    cmds = [\n",
    "        base,\n",
    "        \"kill $(\" + base + \"| awk '{print $2}')\",\n",
    "    ]\n",
    "    for c in cmds:\n",
    "        try:\n",
    "            pipe = subprocess.Popen(c, stdout=subprocess.PIPE)\n",
    "            print(pipe.communicate()[0].decode(\"utf8\"))\n",
    "        except:\n",
    "            print(\"no proces can be killed\")\n",
    "\n",
    "def run_cmd(cmd, timeout=20):\n",
    "    process = subprocess.Popen(args=cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    # output = subprocess.check_output(cmd, stderr=subprocess.STDOUT, timeout=timeout, shell=True)\n",
    "    p = psutil.Process(process.pid)\n",
    "    try:\n",
    "        p.wait(timeout=timeout)\n",
    "    except psutil.TimeoutExpired:\n",
    "        p.kill()\n",
    "        time.sleep(1)\n",
    "        print(\"Kill the timeouted orphan process:\")\n",
    "        kill_orphan_p(\"git clone\")\n",
    "        time.sleep(1)\n",
    "        # need to raise timeout here since process.communicate can reach deadlock after kill_orphan\n",
    "        raise TimeoutError(f\"Timeout!(> {timeout} seconds)\")\n",
    "\n",
    "    stdout, stderr = process.communicate()\n",
    "    return process.returncode, stdout.decode(\"utf-8\"), stderr.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batch: 886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 14287.72it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 12518.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 14216.05it/s]\n",
      " 98%|█████████▊| 98/100 [02:27<00:04,  2.36s/it]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# cmd = 'git clone --depth 1 --no-tags https://github.com/marmolejo/zet.git c:/Users/Administrator/Desktop/verilog_sva_gh_scraper/data/full_repos/583558'\n",
    "clone_dir = 'data/clone_scripts'\n",
    "out_dir = 'data/full_repos'\n",
    "restart_batch = 100\n",
    "finished = False\n",
    "all_scripts = os.listdir(clone_dir)\n",
    "print(\"total batch: {}\".format(len(all_scripts)))\n",
    "while not finished:\n",
    "    try:\n",
    "        for i, file in enumerate(all_scripts):\n",
    "            if i < restart_batch:\n",
    "                continue\n",
    "            if (i+1)%10 == 0:\n",
    "                print(f'at batch no.{i}')\n",
    "            with open(osp.join(clone_dir, file)) as fp:\n",
    "                cmd_repos = fp.readlines()\n",
    "            for cmd_repo in tqdm(cmd_repos):\n",
    "                cmd_repo = cmd_repo.strip()\n",
    "                repo_id = cmd_repo.rsplit('/',1)[-1]\n",
    "                if osp.exists(osp.join(out_dir, repo_id)):\n",
    "                    # print(f'escape {repo_id}')\n",
    "                    continue\n",
    "                retcode, stdout, stderr = run_cmd(cmd_repo, timeout=60)\n",
    "                filter_files_per_repo(out_dir+'/'+repo_id, extensions_to_keep)\n",
    "                if retcode != 0:\n",
    "                    with open('data/failed_repos.txt', 'a') as fp:\n",
    "                        fp.write(repo_id+','+stdout+'\\n'+stderr+'\\n')\n",
    "        finished = True            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Stoped at batch: {}, repor: {}. remove the unfinished repo.\".format(i, repo_id))\n",
    "        try:\n",
    "            shutil.rmtree(osp.join(out_dir, repo_id))\n",
    "        except:\n",
    "            pass\n",
    "        restart_batch = i\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoped at batch: 103\n"
     ]
    }
   ],
   "source": [
    "print(\"Stoped at batch: {}\".format(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. fine-grained repo download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_files_dict(count, content):\n",
    "    global files_dict\n",
    "    files_dict[count] = {\n",
    "        \"path\": content.raw_data['path'],\n",
    "        \"size\": content.raw_data['size'],\n",
    "        \"count_id\": count\n",
    "    }\n",
    "\n",
    "def download_content(repo,content,extensions,out_dir):\n",
    "    content_raw_data = content.raw_data\n",
    "    content_type = content_raw_data['type']\n",
    "    if content_type == 'dir':\n",
    "        pre_github_request_checker()\n",
    "        new_contents = repo.get_contents(content_raw_data['path'])\n",
    "        for new_content in new_contents:\n",
    "            download_content(repo,new_content,extensions,out_dir)\n",
    "    elif content_type == 'file':\n",
    "        extension = get_file_extension(content_raw_data['name'])\n",
    "        if extension in extensions:\n",
    "            global file_count\n",
    "            update_files_dict(file_count,content)\n",
    "            pre_github_request_checker()\n",
    "            try:\n",
    "                with open(os.path.join(out_dir,str(file_count) + \".\" + extension),'wb') as f:\n",
    "                    f.write(content.decoded_content)\n",
    "                file_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Caught exception while trying to write content:\\n{e}\")\n",
    "    # raise Exception(f\"Content type not recognized: {content_type}\")\n",
    "\n",
    "def download_files_from_repo(repo,extensions,out_dir):\n",
    "    global files_dict\n",
    "    global file_count\n",
    "    file_count = 0\n",
    "    files_dict = {}\n",
    "    pre_github_request_checker()\n",
    "    # get file list at root path\n",
    "    contents = repo.get_contents(\"/\")\n",
    "    for content in contents:\n",
    "        download_content(repo,content,extensions,out_dir)\n",
    "    df = pd.DataFrame.from_dict(files_dict,orient='index')\n",
    "    print(f\"Saving csv index with {len(df)} entries\")\n",
    "    df.to_csv(os.path.join(out_dir,\"index.csv\"))\n",
    "    \n",
    "def download_all_repos(df,extensions,out_dir):\n",
    "    all_repo_ids = list(df['id'])\n",
    "    for i in range(0,len(df['id'])):\n",
    "        repo_id = all_repo_ids[i]\n",
    "        pre_github_request_checker()\n",
    "        repo = g.get_repo(repo_id)\n",
    "        repo_dir = os.path.join(out_dir,str(repo_id))\n",
    "        if not os.path.exists(repo_dir):\n",
    "            os.makedirs(repo_dir)\n",
    "        print(f\"Searching repo {i} with id: {repo_id}\")\n",
    "        download_files_from_repo(repo,extensions,repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_all_repos(systemverilog_df, verilog_extension_files, \"./data/full_repos/raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index file\n",
    "\n",
    "This step collects all files into data samples, each file is a code file, which is followed by the dataset splitting step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_df(downloaded_repo_dir,extensions_to_keep):\n",
    "    extensions_map = {ext: True for ext in extensions_to_keep}\n",
    "    df = pd.DataFrame(columns=['directory','repo_id','file_name','extension'])\n",
    "    for repo_id in os.listdir(downloaded_repo_dir):\n",
    "        for root,dirs,files in os.walk(os.path.join(downloaded_repo_dir,repo_id)):\n",
    "            for file in files:\n",
    "                extension = get_file_extension(file)\n",
    "                try:\n",
    "                    if extensions_map[extension]:\n",
    "                        directory = os.path.join(root,file)\n",
    "                        df.loc[len(df)] = [directory, repo_id, file, extension]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    extensions_map[extension] = False\n",
    "        print(f\"Done with repo: {repo_id}\")\n",
    "    print(f\"Extensions: {extensions_map}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verilog_files_df = create_files_df('data/full_repos/permissive',verilog_extension_files + system_verilog_extension_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "verilog_files_df.to_csv('./files_index.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition dataset for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_index = pd.read_csv('data/search_repo_indices/files_index.csv',index_col=0)\n",
    "# files_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314877"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_index = files_index[files_index['extension'].isin(extensions_to_keep)]\n",
    "# files_index = files_index.reset_index(drop=True)\n",
    "len(files_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_index = files_index.reset_index(drop=True)\n",
    "# files_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_indices = np.random.choice(len(files_index),replace=False,size=200)\n",
    "remaining_files_index = files_index.drop(index=few_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314677"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_files_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_partitions = 10\n",
    "tot_len = len(remaining_files_index)\n",
    "for i in range(number_of_partitions):\n",
    "    partition_df = remaining_files_index.iloc[list(range(i*tot_len//number_of_partitions,(i+1)*tot_len//number_of_partitions))]\n",
    "    partition_df.to_csv(f\"data/verilog_partitions/files_index_part_{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_len = len(remaining_files_index)\n",
    "total = 0\n",
    "for i in range(number_of_partitions):\n",
    "    length = len(pd.read_csv(f\"data/verilog_partitions/files_index_part_{i}.csv\"))\n",
    "    total += length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill partitions with source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_source_code(directory):\n",
    "    # Biggest error = utf-8 encoding problem\n",
    "    try:\n",
    "        # return open(directory,'r').read()\n",
    "        with codecs.open(directory,encoding='utf-8', errors='replace', mode = 'r') as f:\n",
    "            data = f.read()\n",
    "        return data.replace(\"\\x00\",\"\") # replacing this might not be needed but someone online said it helps...\n",
    "    except Exception as e:\n",
    "        e_string = f\"0:FOUND ERROR: {e}\"\n",
    "        print(e_string)\n",
    "        return e_string\n",
    "\n",
    "def clean_row_directory(row):\n",
    "    return row['directory'].replace(\"\\\\\",\"/\")\n",
    "\n",
    "def add_source_code_to_index_df(df):\n",
    "    df['directory'] = df.apply(lambda row: clean_row_directory(row),axis=1)\n",
    "    df['code'] = \"\"\n",
    "    tqdm.pandas(desc='Apply read_source_code')\n",
    "    df['code'] = df.progress_apply(lambda row: read_source_code(row['directory']),axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Apply read_source_code:  94%|█████████▎| 29496/31468 [05:13<00:28, 70.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Apply read_source_code: 100%|██████████| 31468/31468 [05:48<00:00, 90.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "number_of_partitions = 10\n",
    "for i in range(number_of_partitions):\n",
    "    df_dir = f\"data/verilog_partitions/files_index_part_{i}.csv\"\n",
    "    print(f\"Starting {i}\")\n",
    "    partition_df = pd.read_csv(df_dir,index_col=0)\n",
    "    new_partition_df = add_source_code_to_index_df(partition_df)\n",
    "    new_partition_df.to_csv(df_dir)\n",
    "    del partition_df, new_partition_df\n",
    "print(\"All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('hdl_dataset_creation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d483179aadbe36b266083fb168142eacd02134ef8f8b2756794bec1efb632f92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 